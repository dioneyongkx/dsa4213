{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72dc409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sentencepiece as spm\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe673bd",
   "metadata": {},
   "source": [
    "#### archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773fc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c9/rl6pcdvd1_xb6ng3n8l9wr2w0000gn/T/ipykernel_14999/1628828404.py:1: DtypeWarning: Columns (7,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full_raw = pd.read_csv('/Users/javier/VSCODE/datahub/enron_data_fraud_labeled.csv')\n"
     ]
    }
   ],
   "source": [
    "# full_raw = pd.read_csv('/Users/javier/VSCODE/datahub/enron_data_fraud_labeled.csv') \n",
    "\n",
    "# filtered_raw = full_raw[['Body','Label']]\n",
    "\n",
    "# filtered_raw.to_csv('full_sample.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a69a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = \"Hi John,  Please check out our new product at https://www.example.com/special-offer.  You can also visit www.testsite.org for more details.  I've attached the latest report as quarterly_results.pdf and also a backup copy as report.docx send to abc@gmail.com and john.doe@gmail.org. Let me know if you have trouble opening summary.xlsx.  Best,  Alice\"\n",
    "\n",
    "# html_text = \"\"\"\n",
    "# <html>\n",
    "#   <head>\n",
    "#     <title>Quarterly Update</title>\n",
    "#   </head>\n",
    "#   <body>\n",
    "#     <h1>Special Offer!</h1>\n",
    "#     <p>Dear customer,</p>\n",
    "    \n",
    "#     <p>\n",
    "#       Please download the latest reports:\n",
    "#       <a href=\"https://example.com/files/quarterly_report.pdf\">Quarterly Report</a>,\n",
    "#       <a href=\"https://example.com/files/summary.docx\">Summary</a>,\n",
    "#       and <a href=\"https://example.com/files/data.xlsx\">Data File</a>.\n",
    "#     </p>\n",
    "\n",
    "#     <p>\n",
    "#       If you cannot access the files, please email \n",
    "#       <a href=\"mailto:support@example.com\">support@example.com</a>.\n",
    "#     </p>\n",
    "\n",
    "#     <p>\n",
    "#       Alternatively, you may contact John at john.doe@workmail.org or visit our site \n",
    "#       <a href=\"http://www.testsite.org\">www.testsite.org</a>.\n",
    "#     </p>\n",
    "\n",
    "#     <p>\n",
    "#       Attached reference documents: <b>budget_2024.pdf</b>, <b>plan_final.docx</b>\n",
    "#     </p>\n",
    "\n",
    "#     <p>\n",
    "#       loveeeeeeeeee\n",
    "#       lovee33333eeee\n",
    "#       a-p-p-l-e\n",
    "#       b.a.n.a.n.a\n",
    "#       fr33 c4$h \n",
    "#       Helloüåç!! This*** is a test üòé #spam @user $100...\n",
    "#       45 46 20000 32323 $222.22\n",
    "#     </p\n",
    "#   </body>\n",
    "# </html>\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1662b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_raw = pd.read_csv('/Users/javier/VSCODE/local/DSA4213_vsc/final_project/test.csv')\n",
    "# print(filtered_raw.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e469e70",
   "metadata": {},
   "source": [
    "#### helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2efab4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking special token\n",
    "def mask_tokens(text):\n",
    "    # replace URLs (http, https, www)\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '<URL>', text)\n",
    "\n",
    "    # replace common file extensions (customize list)\n",
    "    text = re.sub(r'\\b[\\w\\-]+\\.(pdf|docx|xlsx|txt|csv|tar|doc\\.gz|doc)\\b', '<FILE>', text)\n",
    "\n",
    "    # emails\n",
    "    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
    "\n",
    "    # money \n",
    "    text = re.sub(r'\\$\\d+(?:\\.\\d{2})?','<MONEY>',text)\n",
    "\n",
    "    # numbers \n",
    "    text = re.sub(r'\\b\\d+\\b','<NUMBER>',text)\n",
    "    text = text.replace('<NUMBER>', '')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# un HTML raw text \n",
    "def strip_html(raw_html):\n",
    "    \"\"\"\n",
    "    Strip HTML tags, scripts, styles, and normalize whitespace\n",
    "    to return clean raw text from HTML emails.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "\n",
    "    \n",
    "    for a in soup.find_all(\"a\"):\n",
    "        href = a.get(\"href\",None)\n",
    "        if not href:   # skip if no href\n",
    "                continue\n",
    "\n",
    "        # print(a_attribute)\n",
    "\n",
    "        a_attribute = mask_tokens(href)\n",
    "\n",
    "        if a_attribute == '<URL>' : \n",
    "            a.replace_with('<URL>')\n",
    "\n",
    "        elif a_attribute =='<EMAIL>' : \n",
    "            a.replace_with('<EMAIL>')\n",
    "        \n",
    "        elif a_attribute == '<FILE>' : \n",
    "            a.replace_with('<FILE>')\n",
    "\n",
    "        elif a_attribute == '<MONEY>' : \n",
    "            a.replace_with('<MONEY>')\n",
    "        \n",
    "        elif a_attribute == '<NUMBER>' : \n",
    "            a.replace_with('<NUMBER>')\n",
    "\n",
    "    # remove script, style, head, and metadata tags\n",
    "    for tag in soup([\"script\", \"style\", \"head\", \"title\", \"meta\", \"[document]\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # extract text\n",
    "    text = soup.get_text(separator=\" \")\n",
    "\n",
    "    # normalize unicode \n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # replace non-breaking spaces specifically (unicode)\n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "\n",
    "    # collapse all whitespace tokens (line breaks, tabs, multiple spaces) into one space and remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # rim leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# special case handling\n",
    "mapper = str.maketrans({\n",
    "    '0':'o','1':'l','3':'e','4':'a','5':'s','7':'t','$':'s','@':'a'\n",
    "})\n",
    "\n",
    "def deobfuscate_words(text):\n",
    "    \"\"\"\n",
    "    capture non-alphanumeric sequence in windows of 1-3 and replaces with ' ' \n",
    "    l-o-v-e -> l-o , - is detected and removed -> love\n",
    "    \"\"\"\n",
    "    # replace text to number \n",
    "    text = text.translate(mapper)\n",
    "    # remove weird spaces etc \n",
    "    text = re.sub(r'(?i)(?<=\\w)[^A-Za-z0-9\\s]{1,3}(?=\\w)', '', text)\n",
    "    return text\n",
    "\n",
    "def word_capper(text):\n",
    "    text = re.sub(r'(.)\\1{' + str(2) + r',}', lambda m: m.group(1)*2, text)\n",
    "    text = re.sub(r'([!?.,])\\1{1,}', r'\\1\\1', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# whitelist filtering\n",
    "def char_lvl_whitelist_filter(text): \n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\'\\\":;\\-\\_\\(\\)\\@\\#\\$\\%\\^\\&\\<\\>]', '', text)\n",
    "    return text\n",
    "\n",
    "# word level processor \n",
    "def lemmatizer(text) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = ''\n",
    "\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    return sentence.join(lemmatized_words)\n",
    "\n",
    "#final clean\n",
    "def final_punc_removal(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s<>]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "    \n",
    "def preprocess_email_text(raw): \n",
    "    \"\"\"\n",
    "    the whole pipeline of processing\n",
    "    input : dataframe with text column and ham/spam label\n",
    "    output : dataframe with cleaned sentences and ham/spam label\n",
    "    \"\"\"\n",
    "    raw = strip_html(raw) # process html first to capture links from <a> tags\n",
    "    raw = mask_tokens(raw) # mask special tokens \n",
    "    raw = deobfuscate_words(raw)\n",
    "    raw = word_capper(raw)\n",
    "    raw = lemmatizer(raw)\n",
    "    raw = char_lvl_whitelist_filter(raw)\n",
    "    raw = final_punc_removal(raw)\n",
    "    raw = raw.lower()\n",
    "    return raw\n",
    "\n",
    "def preprocess_email_df(df, text_col):\n",
    "    df[text_col] = df[text_col].apply(preprocess_email_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "def vocab_builder(\n",
    "    input_df\n",
    "    ,vocab_size\n",
    "    ,model_type\n",
    ") : \n",
    "    \n",
    "    input_df[\"Body\"].to_csv(\"emails_clean.txt\", index=False, header=False)\n",
    "\n",
    "    # train SentencePiece model\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f\"--input=emails_clean.txt \"\n",
    "        f\"--model_prefix=email_sp \"\n",
    "        f\"--vocab_size={vocab_size} \"\n",
    "        f\"--character_coverage=1.0 \"\n",
    "        f\"--model_type={model_type} \"\n",
    "        f\"--shuffle_input_sentence=false \"\n",
    "        f\"--seed_sentencepiece_size=1000000 \"\n",
    "        f\"--user_defined_symbols=<url>,<email>,<file>,<money>,<pad>\"\n",
    "    )\n",
    "\n",
    "\n",
    "def vocab_to_id_mapper(\n",
    "        input_df\n",
    "        ,max_len\n",
    ") :\n",
    "    \n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(\"email_sp.model\")                 \n",
    "\n",
    "\n",
    "    \n",
    "    MAX_LEN = max_len\n",
    "    pad_id = sp.piece_to_id(\"<pad>\")\n",
    "    if pad_id == -1:  \n",
    "        pad_id = 0\n",
    "\n",
    "    \n",
    "    def encode_ids(text) :\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\" if pd.isna(text) else str(text)\n",
    "        return sp.encode_as_ids(text)\n",
    "\n",
    "    def pad_ids(ids,max_len,pad_id) -> np.ndarray:\n",
    "        if len(ids) >= max_len:\n",
    "            return np.array(ids[:max_len], dtype=np.int32)\n",
    "        return np.array(ids + [pad_id] * (max_len - len(ids)), dtype=np.int32)\n",
    "\n",
    "    \n",
    "    df = input_df.copy()\n",
    "    df[\"sp_ids\"] = df[\"Body\"].apply(encode_ids)\n",
    "\n",
    "    # overwrite sp_ids_padded with NumPy arrays directly\n",
    "    df[\"sp_ids_padded\"] = df[\"sp_ids\"].apply(lambda ids: pad_ids(ids, MAX_LEN, pad_id))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_word2vec(\n",
    "    input_df,\n",
    "    vector_size: int = 300,\n",
    "    window: int = 5,\n",
    "    min_count: int = 5,\n",
    "    epochs: int = 10,\n",
    "    seed: int = 42\n",
    "):\n",
    "    \n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(\"email_sp.model\")\n",
    "\n",
    "    pad_id = sp.piece_to_id(\"<pad>\")\n",
    "    if pad_id == -1:\n",
    "        pad_id = 0\n",
    "\n",
    "    def ids_to_pieces(ids):\n",
    "        return [sp.id_to_piece(i) for i in ids if i != pad_id]\n",
    "\n",
    "    corpus_pieces = [ids_to_pieces(ids) for ids in input_df[\"sp_ids\"]]\n",
    "\n",
    "    w2v = Word2Vec(\n",
    "        sentences=corpus_pieces,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=1,\n",
    "        workers=1,   # keep reproducibility\n",
    "        epochs=epochs,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    return w2v, sp, pad_id\n",
    "\n",
    "\n",
    "def build_embedding_matrix(w2v, sp, pad_id: int, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Build embedding matrix aligned with SentencePiece IDs.\n",
    "    \"\"\"\n",
    "    vocab_size = sp.get_piece_size()\n",
    "    emb_dim = w2v.vector_size\n",
    "\n",
    "    E = np.zeros((vocab_size, emb_dim), dtype=np.float32)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for sp_id in range(vocab_size):\n",
    "        piece = sp.id_to_piece(sp_id)\n",
    "        if piece in w2v.wv:\n",
    "            E[sp_id] = w2v.wv[piece]\n",
    "        else:\n",
    "            E[sp_id] = rng.normal(0.0, 0.01, size=emb_dim).astype(np.float32)\n",
    "\n",
    "    # Keep PAD = 0\n",
    "    if 0 <= pad_id < vocab_size:\n",
    "        E[pad_id] = 0.0\n",
    "\n",
    "    metadata = {\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"pad_id\": pad_id,\n",
    "        \"trained_vocab\": len(w2v.wv),\n",
    "        \"oov_count\": vocab_size - len(w2v.wv),\n",
    "    }\n",
    "    return E, metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2db1f0",
   "metadata": {},
   "source": [
    "#### word embeddign prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92fe7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>status john im not really sure what happened b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Body  Label\n",
       "0  status john im not really sure what happened b...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtered_raw = pd.read_csv('/Users/javier/VSCODE/local/DSA4213_vsc/final_project/test.csv')\n",
    "# debug_test = filtered_raw[['Body','Label']].iloc[:1]\n",
    "# debug_test['Body'] = debug_test['Body'].apply(preprocess_email_text)\n",
    "# print(type(debug_test))\n",
    "# debug_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341154c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean the text , save as variable to enable reuse\n",
    "raw_df = debug_test\n",
    "\n",
    "clean_df = preprocess_email_df(raw_df,'Body')\n",
    "\n",
    "group_keys = [\"Body\"]\n",
    "before = len(clean_df)\n",
    "clean_df = clean_df.drop_duplicates(subset=group_keys,keep=\"first\").reset_index(drop=True)\n",
    "print(f\"dedup removed: {before - len(clean_df)} | New shape: {clean_df.shape}\")\n",
    "# dedup removed: 219240 | New shape: (228177, 3)\n",
    "\n",
    "clean_df.to_csv('../../../raw_data_untracked/clean_df.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a3af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## final project dims \n",
    "# vocab size = 50,000\n",
    "# w2v vector size = 300\n",
    "# max len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ad4c428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228177, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = pd.read_csv('/Users/javier/VSCODE/local/DSA4213_vsc/final_project/raw_data_untracked/clean_df.csv')\n",
    "clean_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7453ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab builder (ONLY CALL ONCE)\n",
    "vocab_builder(clean_df,50_000,42,'bpe') #input, vocab, seed, subword model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb70def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map vocab to id\n",
    "clean_df = pd.read_csv('/Users/javier/VSCODE/local/DSA4213_vsc/final_project/clean_df.csv')\n",
    "\n",
    "mapped_df = vocab_to_id_mapper(clean_df,256) # input , seq len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8da588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call embedder\n",
    "w2v_model, subword_processor, pad_id = train_word2vec(mapped_df)\n",
    "\n",
    "# save model for reload\n",
    "w2v_model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36a8514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sp -> word embedin matrix \n",
    "load_path = 'colab_processed_models/full_220/'\n",
    "\n",
    "\n",
    "# load saved model\n",
    "w2v_model = Word2Vec.load(load_path+\"word2vec.model\")\n",
    "\n",
    "#sentencePiece model & pad_id\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(load_path+\"email_sp.model\")\n",
    "pad_id = sp.piece_to_id(\"<pad>\")\n",
    "if pad_id == -1:\n",
    "    pad_id = 0\n",
    "\n",
    "subword_processor = sp \n",
    "\n",
    "embedding_matrix, embedding_summary = build_embedding_matrix(w2v_model,subword_processor,pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9cc75493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00304717, -0.01039984,  0.00750451, ..., -0.00371915,\n",
       "        -0.01756722,  0.00327995],\n",
       "       [ 0.0172735 , -0.01533861,  0.00863828, ...,  0.00812601,\n",
       "         0.0024166 , -0.01774962],\n",
       "       [ 0.0051541 , -0.00577539,  0.01274447, ..., -0.00862318,\n",
       "         0.00478876, -0.01535642],\n",
       "       ...,\n",
       "       [-0.14897132, -0.13138472, -0.1514239 , ..., -0.13885738,\n",
       "         0.526286  , -0.40069607],\n",
       "       [ 0.14660007,  0.17739818,  0.0375733 , ..., -0.19254978,\n",
       "         0.5717467 , -0.09210922],\n",
       "       [ 0.02365349, -0.05546646,  0.02987715, ..., -0.31438377,\n",
       "         0.66288465, -0.03603499]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "471aed1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50000,\n",
       " 'emb_dim': 300,\n",
       " 'pad_id': 7,\n",
       " 'trained_vocab': 47842,\n",
       " 'oov_count': 2158}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a54a9963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_id = subword_processor.piece_to_id('<pad>')\n",
    "pad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3eeb64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñÅamounts\t0.6620\n",
      "‚ñÅpayment\t0.5578\n",
      "‚ñÅoverdelivery\t0.4874\n",
      "‚ñÅstipulate\t0.4854\n",
      "‚ñÅfmv\t0.4792\n",
      "‚ñÅoutlay\t0.4762\n",
      "‚ñÅunallocated\t0.4653\n",
      "‚ñÅdeterminable\t0.4631\n",
      "‚ñÅaggregate\t0.4616\n",
      "‚ñÅsum\t0.4605\n"
     ]
    }
   ],
   "source": [
    "piece = \"‚ñÅamount\"   # example: a SentencePiece token\n",
    "\n",
    "if piece in w2v_model.wv:\n",
    "    for w, s in w2v_model.wv.most_similar(piece, topn=10):\n",
    "        print(f\"{w}\\t{s:.4f}\")\n",
    "else:\n",
    "    print(\"Piece not in vocab.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd4ea7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñÅamounts\t0.6587\n",
      "‚ñÅpayment\t0.5080\n",
      "‚ñÅsum\t0.4741\n",
      "<money>\t0.4455\n",
      "‚ñÅstipulate\t0.4436\n",
      "‚ñÅexcess\t0.4394\n",
      "‚ñÅquantity\t0.4376\n",
      "‚ñÅperiod\t0.4343\n",
      "‚ñÅpayments\t0.4338\n",
      "‚ñÅaggregate\t0.4327\n"
     ]
    }
   ],
   "source": [
    "piece = \"‚ñÅamount\"   # example: a SentencePiece token\n",
    "\n",
    "if piece in w2v_model.wv:\n",
    "    for w, s in w2v_model.wv.most_similar(piece, topn=10):\n",
    "        print(f\"{w}\\t{s:.4f}\")\n",
    "else:\n",
    "    print(\"Piece not in vocab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a335c",
   "metadata": {},
   "source": [
    "#### encoder data prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "808cbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_encoder_input = pd.read_csv('/Users/javier/VSCODE/local/DSA4213_vsc/final_project/raw_data_untracked/encoder_raw_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d697228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text_combined    object\n",
       "label             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_encoder_input.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8739e52d",
   "metadata": {},
   "source": [
    "##### train val test split\n",
    "\n",
    "split here to ensure both pipeline get same split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "194aef97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dedup removed: 408 | New shape: (82078, 2)\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "group_keys = [\"text_combined\"]\n",
    "before = len(raw_encoder_input)\n",
    "raw_encoder_input = raw_encoder_input.drop_duplicates(subset=group_keys,keep=\"first\").reset_index(drop=True)\n",
    "print(f\"dedup removed: {before - len(raw_encoder_input)} | New shape: {raw_encoder_input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8c6362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_encoder_input.index.values\n",
    "y = raw_encoder_input['label'].astype(int).values\n",
    "\n",
    "# 80% train 20% val+test\n",
    "idx_train, idx_tmp, y_train, y_tmp = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "# 10% val 10% test\n",
    "idx_val, idx_test, y_val, y_test = train_test_split(\n",
    "    idx_tmp, y_tmp, test_size=0.5, stratify=y_tmp, random_state=SEED\n",
    ")\n",
    "\n",
    "train_df = raw_encoder_input.loc[idx_train].reset_index(drop=True)\n",
    "val_df   = raw_encoder_input.loc[idx_val].reset_index(drop=True)\n",
    "test_df  = raw_encoder_input.loc[idx_test].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fd40990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_combined    object\n",
      "label             int64\n",
      "dtype: object\n",
      "text_combined    object\n",
      "label             int64\n",
      "dtype: object\n",
      "text_combined    object\n",
      "label             int64\n",
      "dtype: object\n",
      "(65662, 2)\n",
      "(8208, 2)\n",
      "(8208, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.dtypes)\n",
    "print(val_df.dtypes)\n",
    "print(test_df.dtypes)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9a1e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../encoding/raw_encoder_data_sets/train_set.csv',index=False)\n",
    "val_df.to_csv('../encoding/raw_encoder_data_sets/valid_set.csv',index=False)\n",
    "test_df.to_csv('../encoding/raw_encoder_data_sets/test_set.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bd550b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(set(val_df[\"text_combined\"]) & set(test_df[\"text_combined\"])))\n",
    "print(len(set(val_df[\"text_combined\"]) & set(train_df[\"text_combined\"])))\n",
    "print(len(set(test_df[\"text_combined\"]) & set(train_df[\"text_combined\"])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa4213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
