{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sentencepiece as spm\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe673bd",
   "metadata": {},
   "source": [
    "#### archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_raw = pd.read_csv('/Users/javier/VSCODE/datahub/enron_data_fraud_labeled.csv') \n",
    "\n",
    "# filtered_raw = full_raw[['Body','Label']].iloc[:100_000]\n",
    "\n",
    "# filtered_raw.to_csv('test.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a69a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = \"Hi John,  Please check out our new product at https://www.example.com/special-offer.  You can also visit www.testsite.org for more details.  I've attached the latest report as quarterly_results.pdf and also a backup copy as report.docx send to abc@gmail.com and john.doe@gmail.org. Let me know if you have trouble opening summary.xlsx.  Best,  Alice\"\n",
    "\n",
    "# html_text = \"\"\"\n",
    "# <html>\n",
    "#   <head>\n",
    "#     <title>Quarterly Update</title>\n",
    "#   </head>\n",
    "#   <body>\n",
    "#     <h1>Special Offer!</h1>\n",
    "#     <p>Dear customer,</p>\n",
    "    \n",
    "#     <p>\n",
    "#       Please download the latest reports:\n",
    "#       <a href=\"https://example.com/files/quarterly_report.pdf\">Quarterly Report</a>,\n",
    "#       <a href=\"https://example.com/files/summary.docx\">Summary</a>,\n",
    "#       and <a href=\"https://example.com/files/data.xlsx\">Data File</a>.\n",
    "#     </p>\n",
    "\n",
    "#     <p>\n",
    "#       If you cannot access the files, please email \n",
    "#       <a href=\"mailto:support@example.com\">support@example.com</a>.\n",
    "#     </p>\n",
    "\n",
    "#     <p>\n",
    "#       Alternatively, you may contact John at john.doe@workmail.org or visit our site \n",
    "#       <a href=\"http://www.testsite.org\">www.testsite.org</a>.\n",
    "#     </p>\n",
    "\n",
    "#     <p>\n",
    "#       Attached reference documents: <b>budget_2024.pdf</b>, <b>plan_final.docx</b>\n",
    "#     </p>\n",
    "\n",
    "#     <p>\n",
    "#       loveeeeeeeeee\n",
    "#       lovee33333eeee\n",
    "#       a-p-p-l-e\n",
    "#       b.a.n.a.n.a\n",
    "#       fr33 c4$h \n",
    "#       Helloüåç!! This*** is a test üòé #spam @user $100...\n",
    "#       45 46 20000 32323 $222.22\n",
    "#     </p\n",
    "#   </body>\n",
    "# </html>\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1662b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_raw = pd.read_csv('/Users/javier/VSCODE/local/DSA4213_vsc/final_project/test.csv')\n",
    "print(filtered_raw.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e469e70",
   "metadata": {},
   "source": [
    "#### helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efab4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking special token\n",
    "def mask_tokens(text):\n",
    "    # replace URLs (http, https, www)\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '<URL>', text)\n",
    "\n",
    "    # replace common file extensions (customize list)\n",
    "    text = re.sub(r'\\b[\\w\\-]+\\.(pdf|docx|xlsx|txt|csv|tar|doc\\.gz|doc)\\b', '<FILE>', text)\n",
    "\n",
    "    # emails\n",
    "    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
    "\n",
    "    # money \n",
    "    text = re.sub(r'\\$\\d+(?:\\.\\d{2})?','<MONEY>',text)\n",
    "\n",
    "    # numbers \n",
    "    text = re.sub(r'\\b\\d+\\b','<NUMBER>',text)\n",
    "    text = text.replace('<NUMBER>', '')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# un HTML raw text \n",
    "def strip_html(raw_html):\n",
    "    \"\"\"\n",
    "    Strip HTML tags, scripts, styles, and normalize whitespace\n",
    "    to return clean raw text from HTML emails.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "\n",
    "    \n",
    "    for a in soup.find_all(\"a\"):\n",
    "        a_attribute = a['href'] \n",
    "\n",
    "        a_attribute = mask_tokens(a_attribute)\n",
    "\n",
    "        if a_attribute == '<URL>' : \n",
    "            a.replace_with('<URL>')\n",
    "\n",
    "        elif a_attribute =='<EMAIL>' : \n",
    "            a.replace_with('<EMAIL>')\n",
    "        \n",
    "        elif a_attribute == '<FILE>' : \n",
    "            a.replace_with('<FILE>')\n",
    "\n",
    "        elif a_attribute == '<MONEY>' : \n",
    "            a.replace_with('<MONEY>')\n",
    "        \n",
    "        elif a_attribute == '<NUMBER>' : \n",
    "            a.replace_with('<NUMBER>')\n",
    "\n",
    "    # remove script, style, head, and metadata tags\n",
    "    for tag in soup([\"script\", \"style\", \"head\", \"title\", \"meta\", \"[document]\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # extract text\n",
    "    text = soup.get_text(separator=\" \")\n",
    "\n",
    "    # normalize unicode \n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # replace non-breaking spaces specifically (unicode)\n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "\n",
    "    # collapse all whitespace tokens (line breaks, tabs, multiple spaces) into one space and remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # rim leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# special case handling\n",
    "mapper = str.maketrans({\n",
    "    '0':'o','1':'l','3':'e','4':'a','5':'s','7':'t','$':'s','@':'a'\n",
    "})\n",
    "\n",
    "def deobfuscate_words(text):\n",
    "    \"\"\"\n",
    "    capture non-alphanumeric sequence in windows of 1-3 and replaces with ' ' \n",
    "    l-o-v-e -> l-o , - is detected and removed -> love\n",
    "    \"\"\"\n",
    "    # replace text to number \n",
    "    text = text.translate(mapper)\n",
    "    # remove weird spaces etc \n",
    "    text = re.sub(r'(?i)(?<=\\w)[^A-Za-z0-9\\s]{1,3}(?=\\w)', '', text)\n",
    "    return text\n",
    "\n",
    "def word_capper(text):\n",
    "    text = re.sub(r'(.)\\1{' + str(2) + r',}', lambda m: m.group(1)*2, text)\n",
    "    text = re.sub(r'([!?.,])\\1{1,}', r'\\1\\1', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# whitelist filtering\n",
    "def char_lvl_whitelist_filter(text): \n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\'\\\":;\\-\\_\\(\\)\\@\\#\\$\\%\\^\\&\\<\\>]', '', text)\n",
    "    return text\n",
    "\n",
    "# word level processor \n",
    "def lemmatizer(text) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = ''\n",
    "\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    return sentence.join(lemmatized_words)\n",
    "\n",
    "#final clean\n",
    "def final_punc_removal(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s<>]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2db1f0",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f9927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_email_text(raw): \n",
    "    \"\"\"\n",
    "    the whole pipeline of processing\n",
    "    input : dataframe with text column and ham/spam label\n",
    "    output : dataframe with cleaned sentences and ham/spam label\n",
    "    \"\"\"\n",
    "    raw = strip_html(raw) # process html first to capture links from <a> tags\n",
    "    raw = mask_tokens(raw) # mask special tokens \n",
    "    raw = deobfuscate_words(raw)\n",
    "    raw = word_capper(raw)\n",
    "    raw = lemmatizer(raw)\n",
    "    raw = char_lvl_whitelist_filter(raw)\n",
    "    raw = final_punc_removal(raw)\n",
    "    raw = raw.lower()\n",
    "    return raw\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c92fe7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Body     object\n",
      "Label     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "debug_test = filtered_raw[['Body','Label']].iloc[:1]\n",
    "debug_test['Body'] = debug_test['Body'].apply(preprocess_email_text)\n",
    "print(type(debug_test))\n",
    "print(debug_test.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee358285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_builder(\n",
    "    input_df\n",
    ") : \n",
    "    \n",
    "    input_df[\"Body\"].to_csv(\"emails_clean.txt\", index=False, header=False)\n",
    "\n",
    "    # train SentencePiece model\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        \"--input=emails_clean.txt \"\n",
    "        \"--model_prefix=email_sp \"\n",
    "        \"--vocab_size=40 \"\n",
    "        \"--character_coverage=1.0 \"\n",
    "        \"--user_defined_symbols=<url>,<email>,<file>,<money>\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_to_id_mapper(\n",
    "        input_df\n",
    "        ,max_len\n",
    ") :\n",
    "    # --- 0) Setup ---\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(\"email_sp.model\")                 # your trained SentencePiece model\n",
    "\n",
    "\n",
    "    # --- Parameters ---\n",
    "    MAX_LEN = max_len\n",
    "    pad_id = sp.piece_to_id(\"<pad>\")\n",
    "    if pad_id == -1:   # fallback if no <pad> token defined\n",
    "        pad_id = 0\n",
    "\n",
    "    # --- Helpers ---\n",
    "    def encode_ids() :\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\" if pd.isna(text) else str(text)\n",
    "        return sp.encode_as_ids(text)\n",
    "\n",
    "    def pad_ids(\n",
    "            ids\n",
    "            , max_len\n",
    "            , pad_id) -> np.ndarray:\n",
    "        \"\"\"Return a NumPy array of fixed length.\"\"\"\n",
    "        if len(ids) >= max_len:\n",
    "            return np.array(ids[:max_len], dtype=np.int32)\n",
    "        return np.array(ids + [pad_id] * (max_len - len(ids)), dtype=np.int32)\n",
    "\n",
    "    # --- Apply to DataFrame ---\n",
    "    df = input_df\n",
    "    df[\"sp_ids\"] = df[\"Body\"].apply(encode_ids)\n",
    "\n",
    "    # overwrite sp_ids_padded with NumPy arrays directly\n",
    "    df[\"sp_ids_padded\"] = df[\"sp_ids\"].apply(lambda ids: pad_ids(ids, MAX_LEN, pad_id))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01519b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_embedder(\n",
    "    input_df,\n",
    "\n",
    "    # model param \n",
    "    vector_size: int = 128,\n",
    "    window: int = 5,\n",
    "    min_count: int = 2,\n",
    "    epochs: int = 5,\n",
    "    pad_token: str = \"<pad>\",\n",
    "\n",
    "    # reproducibility\n",
    "    seed: int = 42\n",
    "):\n",
    "\n",
    "\n",
    "    # --- Load SentencePiece ---\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(\"email_sp.model\")\n",
    "    vocab_size = sp.get_piece_size()\n",
    "\n",
    "    # Resolve pad id (fallback to 0 if not present)\n",
    "    pad_id = sp.piece_to_id(pad_token)\n",
    "    if pad_id == -1:\n",
    "        pad_id = 0\n",
    "\n",
    "    # --- Build Word2Vec corpus from UNPADDED ids -> pieces (strings) ---\n",
    "    def ids_to_pieces(ids):\n",
    "        return [sp.id_to_piece(i) for i in ids if i != pad_id]\n",
    "\n",
    "\n",
    "    corpus_pieces = [ids_to_pieces(ids) for ids in input_df[\"sp_ids\"]]\n",
    "\n",
    "    # --- Train Word2Vec on subword pieces ---\n",
    "    w2v = Word2Vec(\n",
    "        sentences=corpus_pieces,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=1,\n",
    "        workers=4,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    # --- Build embedding matrix E aligned to SentencePiece ID order ---\n",
    "    emb_dim = w2v.vector_size\n",
    "\n",
    "    E = np.zeros((vocab_size, emb_dim), dtype=np.float32)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for sp_id in range(vocab_size):\n",
    "\n",
    "        piece = sp.id_to_piece(sp_id)\n",
    "\n",
    "        # map id to wv if exists\n",
    "        if piece in w2v.wv:\n",
    "            E[sp_id] = w2v.wv[piece]\n",
    "        else:\n",
    "            E[sp_id] = rng.normal(0.0, 0.01, size=emb_dim).astype(np.float32)\n",
    "\n",
    "\n",
    "    # Ensure PAD stays zero (common practice)\n",
    "    if 0 <= pad_id < vocab_size:\n",
    "        E[pad_id] = 0.0\n",
    "\n",
    "    metadata= {\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"pad_id\": pad_id,\n",
    "        \"trained_vocab\": len(w2v.wv),\n",
    "        \"oov_count\": vocab_size - len(w2v.wv),\n",
    "    }\n",
    "    return E, w2v, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "341154c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = vocab_to_id_mapper(debug_test,max_len = 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fb70def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.00512243, -0.00813773,  0.00615979, ..., -0.00334885,\n",
       "         0.00162753,  0.00586222],\n",
       "       [ 0.00711227,  0.00793347, -0.00348725, ..., -0.00079718,\n",
       "        -0.01687334, -0.01447112],\n",
       "       ...,\n",
       "       [-0.04319178, -0.01448065,  0.00269867, ..., -0.02073338,\n",
       "         0.02606755,  0.02393069],\n",
       "       [ 0.0009049 ,  0.00228228,  0.02517474, ..., -0.00986857,\n",
       "        -0.00245278,  0.00777338],\n",
       "       [-0.06648282, -0.06284037,  0.02403568, ..., -0.02379413,\n",
       "         0.00048751,  0.00324931]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a,b,c = word2vec_embedder(res,vector_size = 32)\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa4213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
