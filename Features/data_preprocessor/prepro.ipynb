{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sentencepiece as spm\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe673bd",
   "metadata": {},
   "source": [
    "#### archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_raw = pd.read_csv('/Users/javier/VSCODE/datahub/enron_data_fraud_labeled.csv') \n",
    "\n",
    "# filtered_raw = full_raw[['Body','Label']].iloc[:100_000]\n",
    "\n",
    "# filtered_raw.to_csv('test.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a69a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = \"Hi John,  Please check out our new product at https://www.example.com/special-offer.  You can also visit www.testsite.org for more details.  I've attached the latest report as quarterly_results.pdf and also a backup copy as report.docx send to abc@gmail.com and john.doe@gmail.org. Let me know if you have trouble opening summary.xlsx.  Best,  Alice\"\n",
    "\n",
    "# html_text = \"\"\"\n",
    "# <html>\n",
    "#   <head>\n",
    "#     <title>Quarterly Update</title>\n",
    "#   </head>\n",
    "#   <body>\n",
    "#     <h1>Special Offer!</h1>\n",
    "#     <p>Dear customer,</p>\n",
    "    \n",
    "#     <p>\n",
    "#       Please download the latest reports:\n",
    "#       <a href=\"https://example.com/files/quarterly_report.pdf\">Quarterly Report</a>,\n",
    "#       <a href=\"https://example.com/files/summary.docx\">Summary</a>,\n",
    "#       and <a href=\"https://example.com/files/data.xlsx\">Data File</a>.\n",
    "#     </p>\n",
    "\n",
    "#     <p>\n",
    "#       If you cannot access the files, please email \n",
    "#       <a href=\"mailto:support@example.com\">support@example.com</a>.\n",
    "#     </p>\n",
    "\n",
    "#     <p>\n",
    "#       Alternatively, you may contact John at john.doe@workmail.org or visit our site \n",
    "#       <a href=\"http://www.testsite.org\">www.testsite.org</a>.\n",
    "#     </p>\n",
    "\n",
    "#     <p>\n",
    "#       Attached reference documents: <b>budget_2024.pdf</b>, <b>plan_final.docx</b>\n",
    "#     </p>\n",
    "\n",
    "#     <p>\n",
    "#       loveeeeeeeeee\n",
    "#       lovee33333eeee\n",
    "#       a-p-p-l-e\n",
    "#       b.a.n.a.n.a\n",
    "#       fr33 c4$h \n",
    "#       Helloüåç!! This*** is a test üòé #spam @user $100...\n",
    "#       45 46 20000 32323 $222.22\n",
    "#     </p\n",
    "#   </body>\n",
    "# </html>\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1662b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_raw = pd.read_csv('/Users/javier/VSCODE/local/DSA4213_vsc/final_project/test.csv')\n",
    "print(filtered_raw.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e469e70",
   "metadata": {},
   "source": [
    "#### helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efab4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking special token\n",
    "def mask_tokens(text):\n",
    "    # replace URLs (http, https, www)\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '<URL>', text)\n",
    "\n",
    "    # replace common file extensions (customize list)\n",
    "    text = re.sub(r'\\b[\\w\\-]+\\.(pdf|docx|xlsx|txt|csv|tar|doc\\.gz|doc)\\b', '<FILE>', text)\n",
    "\n",
    "    # emails\n",
    "    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
    "\n",
    "    # money \n",
    "    text = re.sub(r'\\$\\d+(?:\\.\\d{2})?','<MONEY>',text)\n",
    "\n",
    "    # numbers \n",
    "    text = re.sub(r'\\b\\d+\\b','<NUMBER>',text)\n",
    "    text = text.replace('<NUMBER>', '')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# un HTML raw text \n",
    "def strip_html(raw_html):\n",
    "    \"\"\"\n",
    "    Strip HTML tags, scripts, styles, and normalize whitespace\n",
    "    to return clean raw text from HTML emails.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "\n",
    "    \n",
    "    for a in soup.find_all(\"a\"):\n",
    "        a_attribute = a['href'] \n",
    "\n",
    "        a_attribute = mask_tokens(a_attribute)\n",
    "\n",
    "        if a_attribute == '<URL>' : \n",
    "            a.replace_with('<URL>')\n",
    "\n",
    "        elif a_attribute =='<EMAIL>' : \n",
    "            a.replace_with('<EMAIL>')\n",
    "        \n",
    "        elif a_attribute == '<FILE>' : \n",
    "            a.replace_with('<FILE>')\n",
    "\n",
    "        elif a_attribute == '<MONEY>' : \n",
    "            a.replace_with('<MONEY>')\n",
    "        \n",
    "        elif a_attribute == '<NUMBER>' : \n",
    "            a.replace_with('<NUMBER>')\n",
    "\n",
    "    # remove script, style, head, and metadata tags\n",
    "    for tag in soup([\"script\", \"style\", \"head\", \"title\", \"meta\", \"[document]\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # extract text\n",
    "    text = soup.get_text(separator=\" \")\n",
    "\n",
    "    # normalize unicode \n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # replace non-breaking spaces specifically (unicode)\n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "\n",
    "    # collapse all whitespace tokens (line breaks, tabs, multiple spaces) into one space and remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # rim leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# special case handling\n",
    "mapper = str.maketrans({\n",
    "    '0':'o','1':'l','3':'e','4':'a','5':'s','7':'t','$':'s','@':'a'\n",
    "})\n",
    "\n",
    "def deobfuscate_words(text):\n",
    "    \"\"\"\n",
    "    capture non-alphanumeric sequence in windows of 1-3 and replaces with ' ' \n",
    "    l-o-v-e -> l-o , - is detected and removed -> love\n",
    "    \"\"\"\n",
    "    # replace text to number \n",
    "    text = text.translate(mapper)\n",
    "    # remove weird spaces etc \n",
    "    text = re.sub(r'(?i)(?<=\\w)[^A-Za-z0-9\\s]{1,3}(?=\\w)', '', text)\n",
    "    return text\n",
    "\n",
    "def word_capper(text):\n",
    "    text = re.sub(r'(.)\\1{' + str(2) + r',}', lambda m: m.group(1)*2, text)\n",
    "    text = re.sub(r'([!?.,])\\1{1,}', r'\\1\\1', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# whitelist filtering\n",
    "def char_lvl_whitelist_filter(text): \n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\'\\\":;\\-\\_\\(\\)\\@\\#\\$\\%\\^\\&\\<\\>]', '', text)\n",
    "    return text\n",
    "\n",
    "# word level processor \n",
    "def lemmatizer(text) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = ''\n",
    "\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    return sentence.join(lemmatized_words)\n",
    "\n",
    "#final clean\n",
    "def final_punc_removal(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s<>]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "    \n",
    "def preprocess_email_text(raw): \n",
    "    \"\"\"\n",
    "    the whole pipeline of processing\n",
    "    input : dataframe with text column and ham/spam label\n",
    "    output : dataframe with cleaned sentences and ham/spam label\n",
    "    \"\"\"\n",
    "    raw = strip_html(raw) # process html first to capture links from <a> tags\n",
    "    raw = mask_tokens(raw) # mask special tokens \n",
    "    raw = deobfuscate_words(raw)\n",
    "    raw = word_capper(raw)\n",
    "    raw = lemmatizer(raw)\n",
    "    raw = char_lvl_whitelist_filter(raw)\n",
    "    raw = final_punc_removal(raw)\n",
    "    raw = raw.lower()\n",
    "    return raw\n",
    "    \n",
    "\n",
    "\n",
    "def vocab_builder(\n",
    "    input_df\n",
    "    ,vocab_size\n",
    "    ,random_seed\n",
    "    ,model_type\n",
    ") : \n",
    "    \n",
    "    input_df[\"Body\"].to_csv(\"emails_clean.txt\", index=False, header=False)\n",
    "\n",
    "    # train SentencePiece model\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f\"--input=emails_clean.txt \"\n",
    "        f\"--model_prefix=email_sp \"\n",
    "        f\"--vocab_size={vocab_size} \"\n",
    "        f\"--character_coverage=1.0 \"\n",
    "        f\"--model_type={model_type} \"\n",
    "        f\"--shuffle_input_sentence=false \"\n",
    "        f\"--seed_sentencepiece_size=1000000 \"\n",
    "        f\"--random_seed={random_seed} \"\n",
    "        f\"--user_defined_symbols=<url>,<email>,<file>,<money>,<pad>\"\n",
    "    )\n",
    "\n",
    "\n",
    "def vocab_to_id_mapper(\n",
    "        input_df\n",
    "        ,max_len\n",
    ") :\n",
    "    \n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(\"email_sp.model\")                 \n",
    "\n",
    "\n",
    "    \n",
    "    MAX_LEN = max_len\n",
    "    pad_id = sp.piece_to_id(\"<pad>\")\n",
    "    if pad_id == -1:  \n",
    "        pad_id = 0\n",
    "\n",
    "    \n",
    "    def encode_ids() :\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\" if pd.isna(text) else str(text)\n",
    "        return sp.encode_as_ids(text)\n",
    "\n",
    "    def pad_ids(ids,max_len,pad_id) -> np.ndarray:\n",
    "        if len(ids) >= max_len:\n",
    "            return np.array(ids[:max_len], dtype=np.int32)\n",
    "        return np.array(ids + [pad_id] * (max_len - len(ids)), dtype=np.int32)\n",
    "\n",
    "    \n",
    "    df = input_df.copy()\n",
    "    df[\"sp_ids\"] = df[\"Body\"].apply(encode_ids)\n",
    "\n",
    "    # overwrite sp_ids_padded with NumPy arrays directly\n",
    "    df[\"sp_ids_padded\"] = df[\"sp_ids\"].apply(lambda ids: pad_ids(ids, MAX_LEN, pad_id))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_word2vec(\n",
    "    input_df,\n",
    "    vector_size: int = 128,\n",
    "    window: int = 5,\n",
    "    min_count: int = 2,\n",
    "    epochs: int = 10,\n",
    "    seed: int = 42\n",
    "):\n",
    "    \n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(\"email_sp.model\")\n",
    "\n",
    "    pad_id = sp.piece_to_id(\"<pad>\")\n",
    "    if pad_id == -1:\n",
    "        pad_id = 0\n",
    "\n",
    "    def ids_to_pieces(ids):\n",
    "        return [sp.id_to_piece(i) for i in ids if i != pad_id]\n",
    "\n",
    "    corpus_pieces = [ids_to_pieces(ids) for ids in input_df[\"sp_ids\"]]\n",
    "\n",
    "    w2v = Word2Vec(\n",
    "        sentences=corpus_pieces,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=1,\n",
    "        workers=1,   # keep reproducibility\n",
    "        epochs=epochs,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    return w2v, sp, pad_id\n",
    "\n",
    "\n",
    "def build_embedding_matrix(w2v, sp, pad_id: int, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Build embedding matrix aligned with SentencePiece IDs.\n",
    "    \"\"\"\n",
    "    vocab_size = sp.get_piece_size()\n",
    "    emb_dim = w2v.vector_size\n",
    "\n",
    "    E = np.zeros((vocab_size, emb_dim), dtype=np.float32)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for sp_id in range(vocab_size):\n",
    "        piece = sp.id_to_piece(sp_id)\n",
    "        if piece in w2v.wv:\n",
    "            E[sp_id] = w2v.wv[piece]\n",
    "        else:\n",
    "            E[sp_id] = rng.normal(0.0, 0.01, size=emb_dim).astype(np.float32)\n",
    "\n",
    "    # Keep PAD = 0\n",
    "    if 0 <= pad_id < vocab_size:\n",
    "        E[pad_id] = 0.0\n",
    "\n",
    "    metadata = {\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"pad_id\": pad_id,\n",
    "        \"trained_vocab\": len(w2v.wv),\n",
    "        \"oov_count\": vocab_size - len(w2v.wv),\n",
    "    }\n",
    "    return E, metadata\n",
    "\n",
    "\n",
    "def zz_word2vec_embedder(\n",
    "    input_df,\n",
    "\n",
    "    # model param \n",
    "    vector_size: int = 128,\n",
    "    window: int = 5,\n",
    "    min_count: int = 2,\n",
    "    epochs: int = 10 ,\n",
    "\n",
    "    # reproducibility\n",
    "    seed: int = 42\n",
    "):\n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(\"email_sp.model\")\n",
    "    vocab_size = sp.get_piece_size()\n",
    "\n",
    "    \n",
    "    pad_id = sp.piece_to_id('<pad>')\n",
    "    if pad_id == -1:\n",
    "        pad_id = 0\n",
    "\n",
    "    \n",
    "    def ids_to_pieces(ids):\n",
    "        return [sp.id_to_piece(i) for i in ids if i != pad_id]\n",
    "\n",
    "\n",
    "    corpus_pieces = [ids_to_pieces(ids) for ids in input_df[\"sp_ids\"]]\n",
    "\n",
    "    \n",
    "    w2v = Word2Vec(\n",
    "        sentences=corpus_pieces,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=1,\n",
    "        workers=1, # for reproductibility\n",
    "        epochs=epochs,\n",
    "        seed = seed\n",
    "    )\n",
    "\n",
    "    \n",
    "    emb_dim = w2v.vector_size\n",
    "\n",
    "    E = np.zeros((vocab_size, emb_dim), dtype=np.float32)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for sp_id in range(vocab_size):\n",
    "\n",
    "        piece = sp.id_to_piece(sp_id)\n",
    "\n",
    "        # map id to wv if exists\n",
    "        if piece in w2v.wv:\n",
    "            E[sp_id] = w2v.wv[piece]\n",
    "        else:\n",
    "            E[sp_id] = rng.normal(0.0, 0.01, size=emb_dim).astype(np.float32)\n",
    "\n",
    "\n",
    "    # Ensure PAD stays zero (common practice)\n",
    "    if 0 <= pad_id < vocab_size:\n",
    "        E[pad_id] = 0.0\n",
    "\n",
    "    metadata= {\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"pad_id\": pad_id,\n",
    "        \"trained_vocab\": len(w2v.wv),\n",
    "        \"oov_count\": vocab_size - len(w2v.wv),\n",
    "    }\n",
    "    return E, w2v, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2db1f0",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92fe7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_test = filtered_raw[['Body','Label']].iloc[:1]\n",
    "# debug_test['Body'] = debug_test['Body'].apply(preprocess_email_text)\n",
    "# print(type(debug_test))\n",
    "# print(debug_test.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341154c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean the text , save as variable to enable reuse\n",
    "# raw_df = debug_test\n",
    "\n",
    "\n",
    "\n",
    "clean_df = preprocess_email_text(raw_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8059ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build vocab \n",
    "# vocab builder (ONLY CALL ONCE)\n",
    "vocab_builder(clean_df,8000,42,'bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb70def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map vocab to id\n",
    "mapped_df = vocab_to_id_mapper(clean_df,256) # this number is the token limit per email\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8da588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call embedder\n",
    "w2v_model, subword_processor, pad_id = train_word2vec(mapped_df)\n",
    "\n",
    "# save model for reload\n",
    "w2v_model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sp -> word embedin matrix \n",
    "# # load saved model\n",
    "# w2v = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# #sentencePiece model & pad_id\n",
    "# import sentencepiece as spm\n",
    "# sp = spm.SentencePieceProcessor()\n",
    "# sp.load(\"email_sp.model\")\n",
    "# pad_id = sp.piece_to_id(\"<pad>\")\n",
    "# if pad_id == -1:\n",
    "#     pad_id = 0\n",
    "\n",
    "\n",
    "embedding_matrix, embedding_summary = build_embedding_matrix(w2v_model,subword_processor,pad_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa4213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
