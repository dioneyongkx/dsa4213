{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biLSTM import BiLSTMClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "import time\n",
    "\n",
    "\n",
    "SEED = 42  # or any integer\n",
    "\n",
    "# Python & NumPy\n",
    "\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# PyTorch (CPU & CUDA)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # if multi-GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e966e61",
   "metadata": {},
   "source": [
    "# Tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd11b1",
   "metadata": {},
   "source": [
    "#### helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "390de247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking special token\n",
    "def mask_tokens(text):\n",
    "    # replace URLs (http, https, www)\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '<URL>', text)\n",
    "\n",
    "    # replace common file extensions (customize list)\n",
    "    text = re.sub(r'\\b[\\w\\-]+\\.(pdf|docx|xlsx|txt|csv|tar|doc\\.gz|doc)\\b', '<FILE>', text)\n",
    "\n",
    "    # emails\n",
    "    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
    "\n",
    "    # money \n",
    "    text = re.sub(r'\\$\\d+(?:\\.\\d{2})?','<MONEY>',text)\n",
    "\n",
    "    # numbers \n",
    "    text = re.sub(r'\\b\\d+\\b','<NUMBER>',text)\n",
    "    text = text.replace('<NUMBER>', '')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# un HTML raw text \n",
    "def strip_html(raw_html):\n",
    "    \"\"\"\n",
    "    Strip HTML tags, scripts, styles, and normalize whitespace\n",
    "    to return clean raw text from HTML emails.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "\n",
    "    \n",
    "    for a in soup.find_all(\"a\"):\n",
    "        href = a.get(\"href\",None)\n",
    "        if not href:   # skip if no href\n",
    "                continue\n",
    "\n",
    "        # print(a_attribute)\n",
    "\n",
    "        a_attribute = mask_tokens(href)\n",
    "\n",
    "        if a_attribute == '<URL>' : \n",
    "            a.replace_with('<URL>')\n",
    "\n",
    "        elif a_attribute =='<EMAIL>' : \n",
    "            a.replace_with('<EMAIL>')\n",
    "        \n",
    "        elif a_attribute == '<FILE>' : \n",
    "            a.replace_with('<FILE>')\n",
    "\n",
    "        elif a_attribute == '<MONEY>' : \n",
    "            a.replace_with('<MONEY>')\n",
    "        \n",
    "        elif a_attribute == '<NUMBER>' : \n",
    "            a.replace_with('<NUMBER>')\n",
    "\n",
    "    # remove script, style, head, and metadata tags\n",
    "    for tag in soup([\"script\", \"style\", \"head\", \"title\", \"meta\", \"[document]\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # extract text\n",
    "    text = soup.get_text(separator=\" \")\n",
    "\n",
    "    # normalize unicode \n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # replace non-breaking spaces specifically (unicode)\n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "\n",
    "    # collapse all whitespace tokens (line breaks, tabs, multiple spaces) into one space and remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # rim leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# special case handling\n",
    "mapper = str.maketrans({\n",
    "    '0':'o','1':'l','3':'e','4':'a','5':'s','7':'t','$':'s','@':'a'\n",
    "})\n",
    "\n",
    "def deobfuscate_words(text):\n",
    "    \"\"\"\n",
    "    capture non-alphanumeric sequence in windows of 1-3 and replaces with ' ' \n",
    "    l-o-v-e -> l-o , - is detected and removed -> love\n",
    "    \"\"\"\n",
    "    # replace text to number \n",
    "    text = text.translate(mapper)\n",
    "    # remove weird spaces etc \n",
    "    text = re.sub(r'(?i)(?<=\\w)[^A-Za-z0-9\\s]{1,3}(?=\\w)', '', text)\n",
    "    return text\n",
    "\n",
    "def word_capper(text):\n",
    "    text = re.sub(r'(.)\\1{' + str(2) + r',}', lambda m: m.group(1)*2, text)\n",
    "    text = re.sub(r'([!?.,])\\1{1,}', r'\\1\\1', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# whitelist filtering\n",
    "def char_lvl_whitelist_filter(text): \n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\'\\\":;\\-\\_\\(\\)\\@\\#\\$\\%\\^\\&\\<\\>]', '', text)\n",
    "    return text\n",
    "\n",
    "# word level processor \n",
    "def lemmatizer(text) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = ''\n",
    "\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    return sentence.join(lemmatized_words)\n",
    "\n",
    "#final clean\n",
    "def final_punc_removal(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s<>]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "    \n",
    "def preprocess_email_text(raw): \n",
    "    \"\"\"\n",
    "    the whole pipeline of processing\n",
    "    input : dataframe with text column and ham/spam label\n",
    "    output : dataframe with cleaned sentences and ham/spam label\n",
    "    \"\"\"\n",
    "    raw = strip_html(raw) # process html first to capture links from <a> tags\n",
    "    raw = mask_tokens(raw) # mask special tokens \n",
    "    raw = deobfuscate_words(raw)\n",
    "    raw = word_capper(raw)\n",
    "    raw = lemmatizer(raw)\n",
    "    raw = char_lvl_whitelist_filter(raw)\n",
    "    raw = final_punc_removal(raw)\n",
    "    raw = raw.lower()\n",
    "    return raw\n",
    "\n",
    "def preprocess_email_df(df, text_col):\n",
    "    df[text_col] = df[text_col].apply(preprocess_email_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "def vocab_builder(\n",
    "    input_df\n",
    "    ,vocab_size\n",
    "    ,model_type\n",
    ") : \n",
    "    \n",
    "    input_df[\"Body\"].to_csv(\"emails_clean.txt\", index=False, header=False)\n",
    "\n",
    "    # train SentencePiece model\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f\"--input=emails_clean.txt \"\n",
    "        f\"--model_prefix=email_sp \"\n",
    "        f\"--vocab_size={vocab_size} \"\n",
    "        f\"--character_coverage=1.0 \"\n",
    "        f\"--model_type={model_type} \"\n",
    "        f\"--shuffle_input_sentence=false \"\n",
    "        f\"--seed_sentencepiece_size=1000000 \"\n",
    "        f\"--user_defined_symbols=<url>,<email>,<file>,<money>,<pad>\"\n",
    "    )\n",
    "\n",
    "\n",
    "#####################################################################################################################\n",
    "def vocab_to_id_mapper(\n",
    "        input_df\n",
    "        ,max_len\n",
    "        ,sp\n",
    ") :\n",
    "    \n",
    "    pad_id = sp.piece_to_id(\"<pad>\")\n",
    "    if pad_id == -1:  \n",
    "        pad_id = 0\n",
    "\n",
    "    \n",
    "    def encode_ids(text) :\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\" if pd.isna(text) else str(text)\n",
    "        return sp.encode_as_ids(text)\n",
    "\n",
    "    def pad_ids(ids,max_len,pad_id) -> np.ndarray:\n",
    "        if len(ids) >= max_len:\n",
    "            return np.array(ids[:max_len], dtype=np.int32)\n",
    "        return np.array(ids + [pad_id] * (max_len - len(ids)), dtype=np.int32)\n",
    "\n",
    "    \n",
    "    df = input_df.copy()\n",
    "    df[\"sp_ids\"] = df[\"Body\"].apply(encode_ids)\n",
    "\n",
    "    # overwrite sp_ids_padded with NumPy arrays directly\n",
    "    df[\"sp_ids_padded\"] = df[\"sp_ids\"].apply(lambda ids: pad_ids(ids, max_len, pad_id))\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_embedding_matrix(w2v, sp, pad_id: int, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Build embedding matrix aligned with SentencePiece IDs.\n",
    "    \"\"\"\n",
    "    vocab_size = sp.get_piece_size()\n",
    "    emb_dim = w2v.vector_size\n",
    "\n",
    "    E = np.zeros((vocab_size, emb_dim), dtype=np.float32)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for sp_id in range(vocab_size):\n",
    "        piece = sp.id_to_piece(sp_id)\n",
    "        if piece in w2v.wv:\n",
    "            E[sp_id] = w2v.wv[piece]\n",
    "        else:\n",
    "            E[sp_id] = rng.normal(0.0, 0.01, size=emb_dim).astype(np.float32)\n",
    "\n",
    "    # Keep PAD = 0\n",
    "    if 0 <= pad_id < vocab_size:\n",
    "        E[pad_id] = 0.0\n",
    "\n",
    "    metadata = {\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"pad_id\": pad_id,\n",
    "        \"trained_vocab\": len(w2v.wv),\n",
    "        \"oov_count\": vocab_size - len(w2v.wv),\n",
    "    }\n",
    "    return E, metadata\n",
    "\n",
    "class TextDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(np.stack(X), dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c816aa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from full_447_batch_A/word2vec.model\n",
      "INFO:gensim.utils:loading wv recursively from full_447_batch_A/word2vec.model.wv.* with mmap=None\n",
      "INFO:gensim.utils:loading vectors from full_447_batch_A/word2vec.model.wv.vectors.npy with mmap=None\n",
      "INFO:gensim.utils:loading syn1neg from full_447_batch_A/word2vec.model.syn1neg.npy with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname': 'full_447_batch_A/word2vec.model', 'datetime': '2025-10-14T16:41:50.601672', 'gensim': '4.3.3', 'python': '3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 08:03:38) [Clang 14.0.6 ]', 'platform': 'macOS-14.5-arm64-arm-64bit', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "# build sp -> word embedin matrix \n",
    "load_path = 'full_447_batch_A/'\n",
    "\n",
    "\n",
    "# load saved model\n",
    "w2v_model = Word2Vec.load(load_path+\"word2vec.model\")\n",
    "\n",
    "#sentencePiece model & pad_id\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(load_path+\"email_sp.model\")\n",
    "pad_id = sp.piece_to_id(\"<pad>\")\n",
    "if pad_id == -1:\n",
    "    pad_id = 0\n",
    "\n",
    "subword_processor = sp \n",
    "\n",
    "embedding_matrix, embedding_summary = build_embedding_matrix(w2v_model,subword_processor,pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5987ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50000,\n",
       " 'emb_dim': 300,\n",
       " 'pad_id': 7,\n",
       " 'trained_vocab': 48242,\n",
       " 'oov_count': 1758}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "394adcef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_id = subword_processor.piece_to_id('<pad>')\n",
    "pad_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414af049",
   "metadata": {},
   "source": [
    "#### Train-valid-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "254dce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_raw = pd.read_csv('../raw_encoder_data_sets/train_set.csv')\n",
    "val_df_raw = pd.read_csv('../raw_encoder_data_sets/valid_set.csv')\n",
    "test_df_raw = pd.read_csv('../raw_encoder_data_sets/test_set.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46b6270e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65662, 2)\n",
      "(8208, 2)\n",
      "(8208, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_df_raw.shape)\n",
    "print(val_df_raw.shape)\n",
    "print(test_df_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78d92aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(set(val_df_raw[\"text_combined\"]) & set(test_df_raw[\"text_combined\"])))\n",
    "print(len(set(val_df_raw[\"text_combined\"]) & set(train_df_raw[\"text_combined\"])))\n",
    "print(len(set(test_df_raw[\"text_combined\"]) & set(train_df_raw[\"text_combined\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4824eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess same as word2vec preprocessing\n",
    "train_df = preprocess_email_df(train_df_raw,'text_combined')\n",
    "val_df = preprocess_email_df(val_df_raw,'text_combined')\n",
    "test_df = preprocess_email_df(test_df_raw,'text_combined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7298f9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = \"text_combined\"\n",
    "\n",
    "# remove overlaps in hierarchical order: Train → Val → Test\n",
    "train_texts = set(train_df[text_col])\n",
    "\n",
    "# remove from val if it appears in train\n",
    "val_df = val_df[~val_df[text_col].isin(train_texts)].reset_index(drop=True)\n",
    "val_texts = set(val_df[text_col])\n",
    "\n",
    "# remove from test if it appears in train or val\n",
    "test_df = test_df[\n",
    "    ~test_df[text_col].isin(train_texts.union(val_texts))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a7e08e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('clean_data_bilstm/train_clean.csv',index=False)\n",
    "val_df.to_csv('clean_data_bilstm/val_clean.csv',index= False)\n",
    "test_df.to_csv('clean_data_bilstm/test_clean.csv',index = False)\n",
    "\n",
    "train_df = pd.read_csv('clean_data_bilstm/train_clean.csv')\n",
    "val_df = pd.read_csv('clean_data_bilstm/val_clean.csv')\n",
    "test_df = pd.read_csv('clean_data_bilstm/test_clean.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "800dfc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65662, 2)\n",
      "(8062, 2)\n",
      "(8056, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b316a4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# check no overlaps across sets\n",
    "print(len(set(train_df[\"text_combined\"]) & set(test_df[\"text_combined\"])))\n",
    "print(len(set(val_df[\"text_combined\"]) & set(test_df[\"text_combined\"])))\n",
    "print(len(set(train_df[\"text_combined\"]) & set(val_df[\"text_combined\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f056d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.rename(columns = {'text_combined':'Body'},inplace=True)\n",
    "val_df.rename(columns = {'text_combined':'Body'},inplace=True)\n",
    "test_df.rename(columns = {'text_combined':'Body'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6806c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise and pad\n",
    "train_df = vocab_to_id_mapper(train_df,256,sp)\n",
    "val_df = vocab_to_id_mapper(val_df,256,sp)\n",
    "test_df = vocab_to_id_mapper(test_df,256,sp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0bda2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch object for model injection\n",
    "\n",
    "train_ds = TextDS(train_df['sp_ids_padded'].values, train_df['label'].values)\n",
    "val_ds   = TextDS(val_df['sp_ids_padded'].values, val_df['label'].values)\n",
    "test_ds  = TextDS(test_df['sp_ids_padded'].values, test_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ad8d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_df['sp_ids_padded'].apply(len).eq(256).all()\n",
    "assert val_df['sp_ids_padded'].apply(len).eq(256).all()\n",
    "assert test_df['sp_ids_padded'].apply(len).eq(256).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1855f543",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f5af9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864481c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_DIR = \"checkpoints\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "def save_ckpt(path, epoch, model, optimizer, scheduler, config, best_f1, seed):\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict() if scheduler is not None else None,\n",
    "        \"config\": config,\n",
    "        \"best_val_f1\": best_f1,\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "    tmp = path + \".tmp\"\n",
    "    torch.save(state, tmp)\n",
    "    os.replace(tmp, path)  # atomic replace\n",
    "\n",
    "def load_ckpt(path, model, optimizer=None, scheduler=None, map_location=\"cpu\"):\n",
    "    ckpt = torch.load(path, map_location=map_location)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    if optimizer is not None and \"optimizer\" in ckpt and ckpt[\"optimizer\"] is not None:\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    if scheduler is not None and \"scheduler\" in ckpt and ckpt[\"scheduler\"] is not None:\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
    "    return ckpt\n",
    "\n",
    "def latest_epoch_path():\n",
    "    files = [f for f in os.listdir(CKPT_DIR) if f.startswith(\"epoch_\") and f.endswith(\".pt\")]\n",
    "    if not files: return None\n",
    "    files.sort()\n",
    "    return os.path.join(CKPT_DIR, files[-1])\n",
    "\n",
    "def evaluate_metrics(dl, model, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            y_true.extend(yb.numpy())\n",
    "            y_pred.extend(preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return p, r, f1, acc\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def load_best_for_inference(manifest_path=\"checkpoints/manifest.json\", map_location=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if map_location is None else map_location\n",
    "    with open(manifest_path) as f:\n",
    "        m = json.load(f)\n",
    "\n",
    "    # rebuild model\n",
    "    embedding_matrix = np.load(m[\"embedding_matrix_file\"])\n",
    "    model = BiLSTMClassifier(\n",
    "        embedding_matrix=embedding_matrix,\n",
    "        pad_id=m[\"pad_id\"],\n",
    "        hidden_dim=m[\"hidden_dim\"],\n",
    "        num_layers=m[\"num_layers\"],\n",
    "        dropout=m[\"dropout\"],\n",
    "        bidirectional=m[\"bidirectional\"],\n",
    "        num_classes=m[\"num_classes\"],\n",
    "    ).to(device)\n",
    "\n",
    "    # load best weights\n",
    "    ckpt = torch.load(m[\"best_ckpt\"], map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    model.eval()\n",
    "    return model, m\n",
    "\n",
    "def get_lr(optim):  \n",
    "    for pg in optim.param_groups:\n",
    "        return pg[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ef37905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True,\n",
    "                      num_workers=2, pin_memory=True,\n",
    "                      worker_init_fn=seed_worker, generator=g)\n",
    "val_dl   = DataLoader(val_ds, batch_size=128, shuffle=False,\n",
    "                      num_workers=2, pin_memory=True,\n",
    "                      worker_init_fn=seed_worker, generator=g)\n",
    "test_dl  = DataLoader(test_ds, batch_size=128, shuffle=False,\n",
    "                      num_workers=2, pin_memory=True,\n",
    "                      worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e69216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:biLSTM:BiLSTM Encoder initialized | emb_dim=300, hidden_dim=256, layers=2, bidirectional=True, freeze_embeddings=True\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"batch_size_train\": 64,\n",
    "    \"batch_size_eval\": 128,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.3,\n",
    "    \"bidirectional\": True,\n",
    "    \"lr\": 1e-3,\n",
    "    \"max_epochs\": 20,\n",
    "    \"patience\": 3,            # early stopping on val F1\n",
    "    \"pad_id\": int(pad_id),\n",
    "    \"max_len\": 256,\n",
    "    \"num_classes\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "# Model/optim/scheduler from your CONFIG\n",
    "model = BiLSTMClassifier(\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    pad_id=CONFIG[\"pad_id\"],\n",
    "    hidden_dim=CONFIG[\"hidden_dim\"],\n",
    "    num_layers=CONFIG[\"num_layers\"],\n",
    "    dropout=CONFIG[\"dropout\"],\n",
    "    bidirectional=CONFIG[\"bidirectional\"],\n",
    "    num_classes=CONFIG[\"num_classes\"],\n",
    ").to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()  # or weighted, as earlier\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=CONFIG[\"lr\"], weight_decay=CONFIG.get(\"weight_decay\", 0.0)\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88799b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_f1 = -1.0\n",
    "best_val_loss = float(\"inf\")\n",
    "start_epoch = 1\n",
    "\n",
    "# resume if a checkpoint exists\n",
    "latest = latest_epoch_path()\n",
    "if latest:\n",
    "    ckpt = load_ckpt(latest, model, optimizer, scheduler, map_location=device)\n",
    "    start_epoch = ckpt[\"epoch\"] + 1\n",
    "    best_f1 = ckpt.get(\"best_val_f1\", -1.0)\n",
    "    print(f\"Resumed from {latest} at epoch {start_epoch-1}, best_val_f1={best_f1:.4f}\")\n",
    "\n",
    "no_improve = 0\n",
    "history = []  \n",
    "\n",
    "for epoch in range(start_epoch, CONFIG[\"max_epochs\"] + 1):\n",
    "    model.train()\n",
    "    # Optional: re-seed per epoch to keep determinism stable across resumes\n",
    "    # torch.manual_seed(CONFIG[\"seed\"] + epoch)\n",
    "    # torch.cuda.manual_seed_all(CONFIG[\"seed\"] + epoch)\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # add tqdm progress bar for training loop\n",
    "    train_pbar = tqdm(train_dl, desc=f\"Epoch {epoch:02d} [Train]\", leave=False)\n",
    "    for xb, yb in train_pbar:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        if \"clip_grad_norm\" in CONFIG and CONFIG[\"clip_grad_norm\"]:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG[\"clip_grad_norm\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_pbar.set_postfix(loss=f\"{loss.item():.4f}\")  # show current batch loss\n",
    "\n",
    "    # compute average train loss for the epoch\n",
    "    train_loss_avg = running_loss / max(1, len(train_dl))\n",
    "\n",
    "    # compute validation loss (in addition to F1)\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_dl, desc=f\"Epoch {epoch:02d} [Val]\", leave=False)\n",
    "        for xb, yb in val_pbar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            vloss = criterion(logits, yb)\n",
    "            val_running_loss += vloss.item()\n",
    "            val_pbar.set_postfix(vloss=f\"{vloss.item():.4f}\")\n",
    "\n",
    "    val_loss_avg = val_running_loss / max(1, len(val_dl))\n",
    "\n",
    "    p, r, val_f1, acc = evaluate_metrics(val_dl, model, device)\n",
    "    val_loss_avg = val_running_loss / max(1, len(val_dl))\n",
    "    scheduler.step(val_loss_avg)\n",
    "    print(f\"epoch {epoch:02d} | train_loss={train_loss_avg:.4f} | val_loss={val_loss_avg:.4f} | \"\n",
    "          f\"P={p:.4f} | R={r:.4f} | F1={val_f1:.4f}\")\n",
    "\n",
    "    # record metrics\n",
    "    history.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": float(train_loss_avg),\n",
    "        \"val_loss\": float(val_loss_avg),\n",
    "        \"val_precision\": float(p),\n",
    "        \"val_recall\": float(r),\n",
    "        \"val_F1\": float(val_f1),\n",
    "        \"val_accuracy\": float(acc),\n",
    "    })\n",
    "\n",
    "    # save per-epoch checkpoint\n",
    "    save_ckpt(os.path.join(CKPT_DIR, f\"epoch_{epoch:02d}.pt\"),\n",
    "              epoch, model, optimizer, scheduler, CONFIG, best_f1, CONFIG[\"seed\"])\n",
    "\n",
    "    # update best model based on F1 (for saving)\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        save_ckpt(os.path.join(CKPT_DIR, \"best.pt\"),\n",
    "                  epoch, model, optimizer, scheduler, CONFIG, best_f1, CONFIG[\"seed\"])\n",
    "\n",
    "    # early stopping based on val loss\n",
    "    if val_loss_avg < best_val_loss - 1e-5:  # small tolerance\n",
    "        best_val_loss = val_loss_avg\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= CONFIG[\"patience\"]:\n",
    "            print(\"Early stopping triggered on validation loss.\")\n",
    "            break\n",
    "\n",
    "# save training results\n",
    "history_df = pd.DataFrame(history)  \n",
    "history_path = os.path.join(CKPT_DIR, \"training_history.csv\")  \n",
    "history_df.to_csv(history_path, index=False) \n",
    "print(\"Saved training history to:\", history_path) \n",
    "\n",
    "# save embedding matrix\n",
    "embedding_path = os.path.join(CKPT_DIR, \"embedding_matrix.npy\")\n",
    "np.save(embedding_path, embedding_matrix)\n",
    "\n",
    "# create manifest\n",
    "manifest = {\n",
    "    \"seed\": CONFIG[\"seed\"],\n",
    "    \"pad_id\": CONFIG[\"pad_id\"],\n",
    "    \"max_len\": CONFIG[\"max_len\"],\n",
    "    \"hidden_dim\": CONFIG[\"hidden_dim\"],\n",
    "    \"num_layers\": CONFIG[\"num_layers\"],\n",
    "    \"dropout\": CONFIG[\"dropout\"],\n",
    "    \"bidirectional\": CONFIG[\"bidirectional\"],\n",
    "    \"num_classes\": CONFIG[\"num_classes\"],\n",
    "    \"embedding_matrix_file\": embedding_path,\n",
    "    \"sp_model_path\": \"email_sp.model\",\n",
    "    \"best_ckpt\": os.path.join(CKPT_DIR, \"best.pt\"),\n",
    "}\n",
    "with open(os.path.join(CKPT_DIR, \"manifest.json\"), \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print(\"Saved manifest and embedding matrix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50960a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # load best before final test\n",
    "# best_ckpt = load_ckpt(os.path.join(CKPT_DIR, \"best.pt\"), model, map_location=device)\n",
    "# print(f\"Loaded best model from epoch {best_ckpt['epoch']} with val_F1={best_ckpt['best_val_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f29957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloader \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635aaf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_auc(dloader):\n",
    "    model.eval()\n",
    "    probs, gold = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dloader:\n",
    "            xb = xb.to(device)\n",
    "            p = F.softmax(model(xb), dim=-1)[:,1].cpu().numpy()\n",
    "            probs.append(p); gold.append(yb.numpy())\n",
    "    return average_precision_score(np.concatenate(gold), np.concatenate(probs))\n",
    "\n",
    "test_f1 = evaluate_f1(test_dl, model, device)\n",
    "test_prauc = pr_auc(test_dl)\n",
    "print(f\"TEST macro-F1={test_f1:.4f}  PR-AUC={test_prauc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa4213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
