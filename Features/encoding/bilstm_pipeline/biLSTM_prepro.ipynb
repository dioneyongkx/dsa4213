{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72dc409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sentencepiece as spm\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe673bd",
   "metadata": {},
   "source": [
    "#### archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a69a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = \"Hi John,  Please check out our new product at https://www.example.com/special-offer.  You can also visit www.testsite.org for more details.  I've attached the latest report as quarterly_results.pdf and also a backup copy as report.docx send to abc@gmail.com and john.doe@gmail.org. Let me know if you have trouble opening summary.xlsx.  Best,  Alice\"\n",
    "\n",
    "# html_text = \"\"\"\n",
    "# <html>\n",
    "#   <head>\n",
    "#     <title>Quarterly Update</title>\n",
    "#   </head>\n",
    "#   <body>\n",
    "#     <h1>Special Offer!</h1>\n",
    "#     <p>Dear customer,</p>\n",
    "    \n",
    "#     <p>\n",
    "#       Please download the latest reports:\n",
    "#       <a href=\"https://example.com/files/quarterly_report.pdf\">Quarterly Report</a>,\n",
    "#       <a href=\"https://example.com/files/summary.docx\">Summary</a>,\n",
    "#       and <a href=\"https://example.com/files/data.xlsx\">Data File</a>.\n",
    "#     </p>\n",
    "\n",
    "#     <p>\n",
    "#       If you cannot access the files, please email \n",
    "#       <a href=\"mailto:support@example.com\">support@example.com</a>.\n",
    "#     </p>\n",
    "\n",
    "#     <p>\n",
    "#       Alternatively, you may contact John at john.doe@workmail.org or visit our site \n",
    "#       <a href=\"http://www.testsite.org\">www.testsite.org</a>.\n",
    "#     </p>\n",
    "\n",
    "#     <p>\n",
    "#       Attached reference documents: <b>budget_2024.pdf</b>, <b>plan_final.docx</b>\n",
    "#     </p>\n",
    "\n",
    "#     <p>\n",
    "#       loveeeeeeeeee\n",
    "#       lovee33333eeee\n",
    "#       a-p-p-l-e\n",
    "#       b.a.n.a.n.a\n",
    "#       fr33 c4$h \n",
    "#       Helloüåç!! This*** is a test üòé #spam @user $100...\n",
    "#       45 46 20000 32323 $222.22\n",
    "#     </p\n",
    "#   </body>\n",
    "# </html>\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e469e70",
   "metadata": {},
   "source": [
    "#### helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2efab4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking special token\n",
    "def mask_tokens(text):\n",
    "    # replace URLs (http, https, www)\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '<URL>', text)\n",
    "\n",
    "    # replace common file extensions (customize list)\n",
    "    text = re.sub(r'\\b[\\w\\-]+\\.(pdf|docx|xlsx|txt|csv|tar|doc\\.gz|doc)\\b', '<FILE>', text)\n",
    "\n",
    "    # emails\n",
    "    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
    "\n",
    "    # money \n",
    "    text = re.sub(r'\\$\\d+(?:\\.\\d{2})?','<MONEY>',text)\n",
    "\n",
    "    # numbers \n",
    "    text = re.sub(r'\\b\\d+\\b','<NUMBER>',text)\n",
    "    text = text.replace('<NUMBER>', '')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# un HTML raw text \n",
    "def strip_html(raw_html):\n",
    "    \"\"\"\n",
    "    Strip HTML tags, scripts, styles, and normalize whitespace\n",
    "    to return clean raw text from HTML emails.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "\n",
    "    \n",
    "    for a in soup.find_all(\"a\"):\n",
    "        href = a.get(\"href\",None)\n",
    "        if not href:   # skip if no href\n",
    "                continue\n",
    "\n",
    "        # print(a_attribute)\n",
    "\n",
    "        a_attribute = mask_tokens(href)\n",
    "\n",
    "        if a_attribute == '<URL>' : \n",
    "            a.replace_with('<URL>')\n",
    "\n",
    "        elif a_attribute =='<EMAIL>' : \n",
    "            a.replace_with('<EMAIL>')\n",
    "        \n",
    "        elif a_attribute == '<FILE>' : \n",
    "            a.replace_with('<FILE>')\n",
    "\n",
    "        elif a_attribute == '<MONEY>' : \n",
    "            a.replace_with('<MONEY>')\n",
    "        \n",
    "        elif a_attribute == '<NUMBER>' : \n",
    "            a.replace_with('<NUMBER>')\n",
    "\n",
    "    # remove script, style, head, and metadata tags\n",
    "    for tag in soup([\"script\", \"style\", \"head\", \"title\", \"meta\", \"[document]\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # extract text\n",
    "    text = soup.get_text(separator=\" \")\n",
    "\n",
    "    # normalize unicode \n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # replace non-breaking spaces specifically (unicode)\n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "\n",
    "    # collapse all whitespace tokens (line breaks, tabs, multiple spaces) into one space and remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # rim leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# special case handling\n",
    "mapper = str.maketrans({\n",
    "    '0':'o','1':'l','3':'e','4':'a','5':'s','7':'t','$':'s','@':'a'\n",
    "})\n",
    "\n",
    "def deobfuscate_words(text):\n",
    "    \"\"\"\n",
    "    capture non-alphanumeric sequence in windows of 1-3 and replaces with ' ' \n",
    "    l-o-v-e -> l-o , - is detected and removed -> love\n",
    "    \"\"\"\n",
    "    # replace text to number \n",
    "    text = text.translate(mapper)\n",
    "    # remove weird spaces etc \n",
    "    text = re.sub(r'(?i)(?<=\\w)[^A-Za-z0-9\\s]{1,3}(?=\\w)', '', text)\n",
    "    return text\n",
    "\n",
    "def word_capper(text):\n",
    "    text = re.sub(r'(.)\\1{' + str(2) + r',}', lambda m: m.group(1)*2, text)\n",
    "    text = re.sub(r'([!?.,])\\1{1,}', r'\\1\\1', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# whitelist filtering\n",
    "def char_lvl_whitelist_filter(text): \n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\'\\\":;\\-\\_\\(\\)\\@\\#\\$\\%\\^\\&\\<\\>]', '', text)\n",
    "    return text\n",
    "\n",
    "# word level processor \n",
    "def lemmatizer(text) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = ''\n",
    "\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    return sentence.join(lemmatized_words)\n",
    "\n",
    "#final clean\n",
    "def final_punc_removal(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s<>]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "    \n",
    "def preprocess_email_text(raw): \n",
    "    \"\"\"\n",
    "    the whole pipeline of processing\n",
    "    input : dataframe with text column and ham/spam label\n",
    "    output : dataframe with cleaned sentences and ham/spam label\n",
    "    \"\"\"\n",
    "    raw = strip_html(raw) # process html first to capture links from <a> tags\n",
    "    raw = mask_tokens(raw) # mask special tokens \n",
    "    raw = deobfuscate_words(raw)\n",
    "    raw = word_capper(raw)\n",
    "    raw = lemmatizer(raw)\n",
    "    raw = char_lvl_whitelist_filter(raw)\n",
    "    raw = final_punc_removal(raw)\n",
    "    raw = raw.lower()\n",
    "    return raw\n",
    "\n",
    "def preprocess_email_df(df, text_col):\n",
    "    df[text_col] = df[text_col].apply(preprocess_email_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "def vocab_builder(\n",
    "    input_df\n",
    "    ,vocab_size\n",
    "    ,model_type\n",
    ") : \n",
    "    \n",
    "    input_df[\"Body\"].to_csv(\"emails_clean.txt\", index=False, header=False)\n",
    "\n",
    "    # train SentencePiece model\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f\"--input=emails_clean.txt \"\n",
    "        f\"--model_prefix=email_sp \"\n",
    "        f\"--vocab_size={vocab_size} \"\n",
    "        f\"--character_coverage=1.0 \"\n",
    "        f\"--model_type={model_type} \"\n",
    "        f\"--shuffle_input_sentence=false \"\n",
    "        f\"--seed_sentencepiece_size=1000000 \"\n",
    "        f\"--user_defined_symbols=<url>,<email>,<file>,<money>,<pad>\"\n",
    "    )\n",
    "\n",
    "\n",
    "def vocab_to_id_mapper(\n",
    "        input_df\n",
    "        ,max_len\n",
    ") :\n",
    "    \n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(\"email_sp.model\")                 \n",
    "\n",
    "\n",
    "    \n",
    "    MAX_LEN = max_len\n",
    "    pad_id = sp.piece_to_id(\"<pad>\")\n",
    "    if pad_id == -1:  \n",
    "        pad_id = 0\n",
    "\n",
    "    \n",
    "    def encode_ids(text) :\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\" if pd.isna(text) else str(text)\n",
    "        return sp.encode_as_ids(text)\n",
    "\n",
    "    def pad_ids(ids,max_len,pad_id) -> np.ndarray:\n",
    "        if len(ids) >= max_len:\n",
    "            return np.array(ids[:max_len], dtype=np.int32)\n",
    "        return np.array(ids + [pad_id] * (max_len - len(ids)), dtype=np.int32)\n",
    "\n",
    "    \n",
    "    df = input_df.copy()\n",
    "    df[\"sp_ids\"] = df[\"Body\"].apply(encode_ids)\n",
    "\n",
    "    # overwrite sp_ids_padded with NumPy arrays directly\n",
    "    df[\"sp_ids_padded\"] = df[\"sp_ids\"].apply(lambda ids: pad_ids(ids, MAX_LEN, pad_id))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_word2vec(\n",
    "    input_df,\n",
    "    vector_size: int = 300,\n",
    "    window: int = 5,\n",
    "    min_count: int = 5,\n",
    "    epochs: int = 10,\n",
    "    seed: int = 42\n",
    "):\n",
    "    \n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(\"email_sp.model\")\n",
    "\n",
    "    pad_id = sp.piece_to_id(\"<pad>\")\n",
    "    if pad_id == -1:\n",
    "        pad_id = 0\n",
    "\n",
    "    def ids_to_pieces(ids):\n",
    "        return [sp.id_to_piece(i) for i in ids if i != pad_id]\n",
    "\n",
    "    corpus_pieces = [ids_to_pieces(ids) for ids in input_df[\"sp_ids\"]]\n",
    "\n",
    "    w2v = Word2Vec(\n",
    "        sentences=corpus_pieces,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=1,\n",
    "        workers=1,   # keep reproducibility\n",
    "        epochs=epochs,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    return w2v, sp, pad_id\n",
    "\n",
    "\n",
    "def build_embedding_matrix(w2v, sp, pad_id: int, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Build embedding matrix aligned with SentencePiece IDs.\n",
    "    \"\"\"\n",
    "    vocab_size = sp.get_piece_size()\n",
    "    emb_dim = w2v.vector_size\n",
    "\n",
    "    E = np.zeros((vocab_size, emb_dim), dtype=np.float32)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for sp_id in range(vocab_size):\n",
    "        piece = sp.id_to_piece(sp_id)\n",
    "        if piece in w2v.wv:\n",
    "            E[sp_id] = w2v.wv[piece]\n",
    "        else:\n",
    "            E[sp_id] = rng.normal(0.0, 0.01, size=emb_dim).astype(np.float32)\n",
    "\n",
    "    # Keep PAD = 0\n",
    "    if 0 <= pad_id < vocab_size:\n",
    "        E[pad_id] = 0.0\n",
    "\n",
    "    metadata = {\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"pad_id\": pad_id,\n",
    "        \"trained_vocab\": len(w2v.wv),\n",
    "        \"oov_count\": vocab_size - len(w2v.wv),\n",
    "    }\n",
    "    return E, metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2db1f0",
   "metadata": {},
   "source": [
    "#### Word2Vec dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385b9097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447417, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_csv('../../../datasets/word2vec_dataset/raw/word2vec_raw.csv')\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341154c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean the text , save as variable to enable reuse\n",
    "clean_df = preprocess_email_df(raw_df,'Body')\n",
    "\n",
    "group_keys = [\"Body\"]\n",
    "before = len(clean_df)\n",
    "clean_df = clean_df.drop_duplicates(subset=group_keys,keep=\"first\").reset_index(drop=True)\n",
    "print(f\"dedup removed: {before - len(clean_df)} | New shape: {clean_df.shape}\")\n",
    "# dedup removed: 219240 | New shape: (228177, 3)\n",
    "\n",
    "clean_df.to_csv('../../../datasets/word2vec_dataset/raw/word2vec_clean.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ad4c428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228177, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = pd.read_csv('../../../datasets/word2vec_dataset/clean/word2vec_clean.csv')\n",
    "clean_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7453ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab builder (ONLY CALL ONCE)\n",
    "vocab_builder(clean_df,50_000,42,'bpe') #input, vocab, seed, subword model\n",
    "\n",
    "sp_model = \"email_sp.model\"\n",
    "sp_vocab = \"email_sp.vocab\"\n",
    "dst_dir = \"embedder_files\" \n",
    "\n",
    "# make sure destination exists\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "# move the files\n",
    "shutil.move(sp_model, os.path.join(dst_dir, sp_model))\n",
    "shutil.move(sp_vocab, os.path.join(dst_dir, sp_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb70def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map vocab to id\n",
    "mapped_df = vocab_to_id_mapper(clean_df,256) # input , seq len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8da588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call embedder\n",
    "w2v_model, subword_processor, pad_id = train_word2vec(mapped_df)\n",
    "\n",
    "# save model for reload\n",
    "w2v_model.save('embedder_files/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a8514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sp -> word embedin matrix \n",
    "# load_path = 'colab_processed_models/full_220/'\n",
    "load_path = 'embedder_files/'\n",
    "\n",
    "# load saved model\n",
    "w2v_model = Word2Vec.load(load_path+\"word2vec.model\")\n",
    "\n",
    "#sentencePiece model & pad_id\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(load_path+\"email_sp.model\")\n",
    "pad_id = sp.piece_to_id(\"<pad>\")\n",
    "if pad_id == -1:\n",
    "    pad_id = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75132458",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, embedding_summary = build_embedding_matrix(w2v_model,sp,pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cc75493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00304717, -0.01039984,  0.00750451, ..., -0.00371915,\n",
       "        -0.01756722,  0.00327995],\n",
       "       [ 0.0172735 , -0.01533861,  0.00863828, ...,  0.00812601,\n",
       "         0.0024166 , -0.01774962],\n",
       "       [ 0.0051541 , -0.00577539,  0.01274447, ..., -0.00862318,\n",
       "         0.00478876, -0.01535642],\n",
       "       ...,\n",
       "       [-0.14897132, -0.13138472, -0.1514239 , ..., -0.13885738,\n",
       "         0.526286  , -0.40069607],\n",
       "       [ 0.14660007,  0.17739818,  0.0375733 , ..., -0.19254978,\n",
       "         0.5717467 , -0.09210922],\n",
       "       [ 0.02365349, -0.05546646,  0.02987715, ..., -0.31438377,\n",
       "         0.66288465, -0.03603499]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "471aed1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50000,\n",
       " 'emb_dim': 300,\n",
       " 'pad_id': 7,\n",
       " 'trained_vocab': 47842,\n",
       " 'oov_count': 2158}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a54a9963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_id = sp.piece_to_id('<pad>')\n",
    "pad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3eeb64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñÅamounts\t0.6620\n",
      "‚ñÅpayment\t0.5578\n",
      "‚ñÅoverdelivery\t0.4874\n",
      "‚ñÅstipulate\t0.4854\n",
      "‚ñÅfmv\t0.4792\n",
      "‚ñÅoutlay\t0.4762\n",
      "‚ñÅunallocated\t0.4653\n",
      "‚ñÅdeterminable\t0.4631\n",
      "‚ñÅaggregate\t0.4616\n",
      "‚ñÅsum\t0.4605\n"
     ]
    }
   ],
   "source": [
    "piece = \"‚ñÅamount\"   # example: a SentencePiece token\n",
    "\n",
    "if piece in w2v_model.wv:\n",
    "    for w, s in w2v_model.wv.most_similar(piece, topn=10):\n",
    "        print(f\"{w}\\t{s:.4f}\")\n",
    "else:\n",
    "    print(\"Piece not in vocab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c2f658",
   "metadata": {},
   "source": [
    "#### Encoder dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac67c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_train_raw = pd.read_csv('../../../datasets/encoder_dataset/raw/train_set.csv')\n",
    "encoder_val_raw = pd.read_csv('../../../datasets/encoder_dataset/raw/valid_set.csv')\n",
    "encoder_test_raw = pd.read_csv('../../../datasets/encoder_dataset/raw/test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1654db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_train_raw.shape)\n",
    "print(encoder_val_raw.shape)\n",
    "print(encoder_test_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d14a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(encoder_val_raw[\"text_combined\"]) & set(encoder_test_raw[\"text_combined\"])))\n",
    "print(len(set(encoder_val_raw[\"text_combined\"]) & set(encoder_train_raw[\"text_combined\"])))\n",
    "print(len(set(encoder_test_raw[\"text_combined\"]) & set(encoder_train_raw[\"text_combined\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf63361",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocess_email_df(encoder_train_raw,'text_combined')\n",
    "val_df = preprocess_email_df(encoder_val_raw,'text_combined')\n",
    "test_df = preprocess_email_df(encoder_test_raw,'text_combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a071d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = \"text_combined\"\n",
    "\n",
    "# remove overlaps in hierarchical order: Train ‚Üí Val ‚Üí Test\n",
    "train_texts = set(train_df[text_col])\n",
    "\n",
    "# remove from val if it appears in train\n",
    "val_df = val_df[~val_df[text_col].isin(train_texts)].reset_index(drop=True)\n",
    "val_texts = set(val_df[text_col])\n",
    "\n",
    "# remove from test if it appears in train or val\n",
    "test_df = test_df[\n",
    "    ~test_df[text_col].isin(train_texts.union(val_texts))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76fbf76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65662, 2)\n",
      "(8062, 2)\n",
      "(8056, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4b2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../../../datasets/encoder_dataset/clean/bilstm/encoder_train_clean.csv',index=False)\n",
    "val_df.to_csv('../../../datasets/encoder_dataset/clean/bilstm/encoder_valid_clean.csv',index=False)\n",
    "test_df.to_csv('../../../datasets/encoder_dataset/clean/bilstm/encoder_test_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a4d3e",
   "metadata": {},
   "source": [
    "#### Cross domain dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ef24aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_dom_raw = pd.read_csv('../../../datasets/cross_domain_dataset/raw/cross_domain_raw.csv')\n",
    "cross_dom_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4b16334",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_dom_raw['cleaned'] = cross_dom_raw['original_text'].apply(preprocess_email_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "987fc0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dedup removed: 479 | New shape: (5093, 3)\n"
     ]
    }
   ],
   "source": [
    "group_keys = [\"cleaned\"]\n",
    "before = len(cross_dom_raw)\n",
    "cross_dom_clean = cross_dom_raw.drop_duplicates(subset=group_keys,keep=\"first\").reset_index(drop=True)\n",
    "print(f\"dedup removed: {before - len(cross_dom_clean)} | New shape: {cross_dom_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8414eaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in a wkly comp to win fa cup final ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088</th>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>this is the 2nd time we have tried contact u u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5089</th>\n",
       "      <td>Will √å_ b going to esplanade fr home?</td>\n",
       "      <td>will b going to esplanade fr home</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>pity was in mood for that soany other suggestions</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>the guy did some bitching but i acted like id ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5092</th>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>rofl its true to its name</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5093 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          original_text  \\\n",
       "0     Go until jurong point, crazy.. Available only ...   \n",
       "1                         Ok lar... Joking wif u oni...   \n",
       "2     Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3     U dun say so early hor... U c already then say...   \n",
       "4     Nah I don't think he goes to usf, he lives aro...   \n",
       "...                                                 ...   \n",
       "5088  This is the 2nd time we have tried 2 contact u...   \n",
       "5089              Will √å_ b going to esplanade fr home?   \n",
       "5090  Pity, * was in mood for that. So...any other s...   \n",
       "5091  The guy did some bitching but I acted like i'd...   \n",
       "5092                         Rofl. Its true to its name   \n",
       "\n",
       "                                                cleaned  label  \n",
       "0     go until jurong point crazy available only in ...      0  \n",
       "1                               ok lar joking wif u oni      0  \n",
       "2     free entry in a wkly comp to win fa cup final ...      1  \n",
       "3           u dun say so early hor u c already then say      0  \n",
       "4     nah i dont think he goes to usf he lives aroun...      0  \n",
       "...                                                 ...    ...  \n",
       "5088  this is the 2nd time we have tried contact u u...      1  \n",
       "5089                  will b going to esplanade fr home      0  \n",
       "5090  pity was in mood for that soany other suggestions      0  \n",
       "5091  the guy did some bitching but i acted like id ...      0  \n",
       "5092                          rofl its true to its name      0  \n",
       "\n",
       "[5093 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_dom_clean = cross_dom_clean[['original_text','cleaned','label']]\n",
    "# cross_dom_clean.to_csv('../../../raw_data_untracked/cross_domain_clean.csv',index = False)\n",
    "cross_dom_clean.to_csv('../../../datasets/cross_domain_dataset/clean/bilstm/cross_domain_clean.csv',index = False)\n",
    "cross_dom_clean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa4213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
