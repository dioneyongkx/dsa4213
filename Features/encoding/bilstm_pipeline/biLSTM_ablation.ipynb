{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d94f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, torch, numpy as np\n",
    "import os\n",
    "from biLSTM import BiLSTMEncoder\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b7041",
   "metadata": {},
   "source": [
    "# helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcdd0166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f34b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt_dir = 'best_ckpts'\n",
    "\n",
    "def f1_with_best_threshold(y_true, proba, average=\"binary\"):\n",
    "    \"\"\"\n",
    "    Sweep thresholds on the *external val set* to pick the best F1.\n",
    "    Returns (best_f1, best_threshold).\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0.05, 0.95, 19)\n",
    "    best_f1, best_t = -1.0, 0.5\n",
    "    for t in thresholds:\n",
    "        y_pred = (proba >= t).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred, average=average)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    return best_f1, best_t\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(encoder, dl, device):\n",
    "    encoder.eval()\n",
    "    feats, labels = [], []\n",
    "    for xb, yb in dl:\n",
    "        xb = xb.to(device)\n",
    "        z  = encoder(xb)                                # [B, feat_dim] (e.g., 512)\n",
    "        feats.append(z.cpu().numpy().astype(np.float32))\n",
    "        labels.append(yb.numpy().astype(np.int64))\n",
    "    return np.concatenate(feats), np.concatenate(labels)\n",
    "\n",
    "def vocab_to_id_mapper(\n",
    "        input_df\n",
    "        ,max_len\n",
    "        ,sp\n",
    ") :\n",
    "    \n",
    "    pad_id = sp.piece_to_id(\"<pad>\")\n",
    "    if pad_id == -1:  \n",
    "        pad_id = 0\n",
    "\n",
    "    \n",
    "    def encode_ids(text) :\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\" if pd.isna(text) else str(text)\n",
    "        return sp.encode_as_ids(text)\n",
    "\n",
    "    def pad_ids(ids,max_len,pad_id) -> np.ndarray:\n",
    "        if len(ids) >= max_len:\n",
    "            return np.array(ids[:max_len], dtype=np.int32)\n",
    "        return np.array(ids + [pad_id] * (max_len - len(ids)), dtype=np.int32)\n",
    "\n",
    "    \n",
    "    df = input_df.copy()\n",
    "    df[\"sp_ids\"] = df[\"Body\"].apply(encode_ids)\n",
    "\n",
    "    # overwrite sp_ids_padded with NumPy arrays directly\n",
    "    df[\"sp_ids_padded\"] = df[\"sp_ids\"].apply(lambda ids: pad_ids(ids, max_len, pad_id))\n",
    "\n",
    "    return df\n",
    "\n",
    "def df_to_ids_and_labels(df):\n",
    "    # 'sp_ids_padded' should be a list/array per row; stack to [N, max_len]\n",
    "    X_ids = np.stack(df[\"sp_ids_padded\"].values).astype(np.int64)\n",
    "    y = df[\"label\"].astype(np.int64).values\n",
    "    return X_ids, y\n",
    "\n",
    "\n",
    "\n",
    "def make_loader(X_ids, y, batch_size=128, shuffle=False):\n",
    "    X = torch.tensor(X_ids, dtype=torch.long)\n",
    "    y = torch.tensor(y,     dtype=torch.long)\n",
    "    ds = TensorDataset(X, y)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69b2ff7",
   "metadata": {},
   "source": [
    "# reloading model from saved checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c18419b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded manifest from: best_ckpts/manifest.json\n",
      "Embedding matrix shape: (50000, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8652/53942971.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(manifest[\"best_ckpt\"], map_location=device)\n",
      "INFO:biLSTM:BiLSTM Encoder initialized | emb_dim=300, hidden_dim=256, layers=2, bidirectional=True, freeze_embeddings=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Encoder reloaded and frozen.\n"
     ]
    }
   ],
   "source": [
    "# load manifest + embedding matrix\n",
    "# CKPT_DIR = \"checkpoints\"\n",
    "MANIFEST_PATH = os.path.join( best_ckpt_dir, \"manifest.json\")\n",
    "\n",
    "with open(MANIFEST_PATH, \"r\") as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "embedding_matrix = np.load(manifest[\"embedding_matrix_file\"])\n",
    "\n",
    "print(f\"Loaded manifest from: {MANIFEST_PATH}\")\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "\n",
    "# load checkpoint and extract encoder weights\n",
    "ckpt = torch.load(manifest[\"best_ckpt\"], map_location=device)\n",
    "state_dict = ckpt[\"model\"]  \n",
    "\n",
    "# If model was saved with full classifier, extract encoder weights only\n",
    "if any(k.startswith(\"encoder.\") for k in state_dict.keys()):\n",
    "    encoder_state = {\n",
    "        k.replace(\"encoder.\", \"\"): v\n",
    "        for k, v in state_dict.items()\n",
    "        if k.startswith(\"encoder.\")\n",
    "    }\n",
    "else:\n",
    "    # already encoder-only (safety fallback)\n",
    "    encoder_state = state_dict\n",
    "\n",
    "# rebuild and load encoder\n",
    "encoder = BiLSTMEncoder(\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    pad_id=manifest[\"pad_id\"],\n",
    "    hidden_dim=manifest[\"hidden_dim\"],\n",
    "    num_layers=manifest[\"num_layers\"],\n",
    "    dropout=0.0,  # irrelevant once frozen\n",
    "    bidirectional=manifest[\"bidirectional\"],\n",
    "    freeze_embeddings=True,\n",
    ").to(device)\n",
    "\n",
    "encoder.load_state_dict(encoder_state, strict=True)\n",
    "encoder.eval()\n",
    "for p in encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"✅ Encoder reloaded and frozen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed19ba1",
   "metadata": {},
   "source": [
    "# tokenising and extracting features for HistGradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e0e0c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = 'embedder_files/'\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(load_path+\"email_sp.model\")\n",
    "pad_id = sp.piece_to_id(\"<pad>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7a6dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../../../datasets/encoder_dataset/clean/bilstm/encoder_train_clean.csv')\n",
    "val_df = pd.read_csv('../../../datasets/encoder_dataset/clean/bilstm/encoder_valid_clean.csv')\n",
    "test_df = pd.read_csv('../../../datasets/encoder_dataset/clean/bilstm/encoder_test_clean.csv')\n",
    "\n",
    "print(len(set(train_df[\"text_combined\"]) & set(test_df[\"text_combined\"])))\n",
    "print(len(set(val_df[\"text_combined\"]) & set(test_df[\"text_combined\"])))\n",
    "print(len(set(train_df[\"text_combined\"]) & set(val_df[\"text_combined\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70a4fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df.rename(columns = {'text_combined':'Body'},inplace=True)\n",
    "val_df.rename(columns = {'text_combined':'Body'},inplace=True)\n",
    "test_df.rename(columns = {'text_combined':'Body'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16a416c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = vocab_to_id_mapper(train_df,256,sp)\n",
    "val_df = vocab_to_id_mapper(val_df,256,sp)\n",
    "test_df = vocab_to_id_mapper(test_df,256,sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16781c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Body             object\n",
       "label             int64\n",
       "sp_ids           object\n",
       "sp_ids_padded    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2e729a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_ids, ytr = df_to_ids_and_labels(train_df)\n",
    "Xva_ids, yva = df_to_ids_and_labels(val_df)\n",
    "Xte_ids, yte = df_to_ids_and_labels(test_df)\n",
    "\n",
    "\n",
    "train_dl = make_loader(Xtr_ids, ytr, batch_size=128, shuffle=False)\n",
    "val_dl   = make_loader(Xva_ids, yva, batch_size=128, shuffle=False)\n",
    "test_dl  = make_loader(Xte_ids, yte, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e3d50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IDs shape: (65662, 256)\n",
      "Val IDs shape: (8062, 256)\n",
      "Test IDs shape: (8056, 256)\n",
      "Train labels: [31386 34276]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train IDs shape:\", Xtr_ids.shape)\n",
    "print(\"Val IDs shape:\", Xva_ids.shape)\n",
    "print(\"Test IDs shape:\", Xte_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f9c64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr = extract_features(encoder, train_dl, device)\n",
    "X_va, y_va = extract_features(encoder, val_dl,   device)\n",
    "X_te, y_te = extract_features(encoder, test_dl,  device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9789b68",
   "metadata": {},
   "source": [
    "# simple hyper param tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8d9cb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search (HGB): 100%|██████████████████████████████████████████████| 8/8 [02:54<00:00, 21.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best (val): {'learning_rate': 0.05, 'max_depth': 4, 'l2_regularization': 0.0, 'min_samples_leaf': 20, 'max_iter': 600} | F1=0.9911 | thr=0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tiny tuning grid (8 configs total)\n",
    "param_grid = [\n",
    "    {\"learning_rate\": 0.03, \"max_depth\": 4, \"l2_regularization\": 0.0, \"min_samples_leaf\": 20, \"max_iter\": 600},\n",
    "    {\"learning_rate\": 0.03, \"max_depth\": 4, \"l2_regularization\": 1.0, \"min_samples_leaf\": 50, \"max_iter\": 600},\n",
    "    {\"learning_rate\": 0.03, \"max_depth\": 6, \"l2_regularization\": 0.0, \"min_samples_leaf\": 20, \"max_iter\": 600},\n",
    "    {\"learning_rate\": 0.03, \"max_depth\": 6, \"l2_regularization\": 1.0, \"min_samples_leaf\": 50, \"max_iter\": 600},\n",
    "\n",
    "    {\"learning_rate\": 0.05, \"max_depth\": 4, \"l2_regularization\": 0.0, \"min_samples_leaf\": 20, \"max_iter\": 600},\n",
    "    {\"learning_rate\": 0.05, \"max_depth\": 4, \"l2_regularization\": 1.0, \"min_samples_leaf\": 50, \"max_iter\": 600},\n",
    "    {\"learning_rate\": 0.05, \"max_depth\": 6, \"l2_regularization\": 0.0, \"min_samples_leaf\": 20, \"max_iter\": 600},\n",
    "    {\"learning_rate\": 0.05, \"max_depth\": 6, \"l2_regularization\": 1.0, \"min_samples_leaf\": 50, \"max_iter\": 600},\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "best_cfg, best_model, best_val_f1, best_thr = None, None, -1.0, 0.5\n",
    "tuning_log = []\n",
    "\n",
    "for cfg in tqdm(param_grid, desc=\"Grid Search (HGB)\", ncols=100):\n",
    "    hgb = HistGradientBoostingClassifier(\n",
    "        **cfg,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=20,\n",
    "        validation_fraction=0.05,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    hgb.fit(X_tr, y_tr)\n",
    "    proba_va = hgb.predict_proba(X_va)[:, 1]\n",
    "    f1_va, thr = f1_with_best_threshold(y_va, proba_va)\n",
    "\n",
    "    tuning_log.append({**cfg, \"val_F1\": f1_va, \"thr\": thr})\n",
    "\n",
    "    if f1_va > best_val_f1:\n",
    "        best_cfg, best_model, best_val_f1, best_thr = cfg, hgb, f1_va, thr\n",
    "\n",
    "print(f\"✅ Best (val): {best_cfg} | F1={best_val_f1:.4f} | thr={best_thr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d420f95",
   "metadata": {},
   "source": [
    "# retraining on best config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58b6738d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] F1=0.9906  P=0.9911  R=0.9902  (thr=0.950)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9889    0.9900    0.9895      3795\n",
      "           1     0.9911    0.9902    0.9906      4267\n",
      "\n",
      "    accuracy                         0.9901      8062\n",
      "   macro avg     0.9900    0.9901    0.9900      8062\n",
      "weighted avg     0.9901    0.9901    0.9901      8062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training best config for histboost \n",
    "\n",
    "# Train on TRAIN ONLY, using the chosen best config\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    **best_cfg,\n",
    "    early_stopping=True, n_iter_no_change=30, validation_fraction=0.1,\n",
    "    random_state=SEED\n",
    ")\n",
    "hgb.fit(X_tr, y_tr)\n",
    "\n",
    "proba_va = hgb.predict_proba(X_va)[:, 1]\n",
    "val_f1, val_thr = f1_with_best_threshold(y_va, proba_va)\n",
    "y_hat_va = (proba_va >= val_thr).astype(int)\n",
    "p, r, f1, _ = precision_recall_fscore_support(y_va, y_hat_va, average=\"binary\")\n",
    "\n",
    "print(f\"[VAL] F1={f1:.4f}  P={p:.4f}  R={r:.4f}  (thr={val_thr:.3f})\")\n",
    "print(classification_report(y_va, y_hat_va, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c989d7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model → best_ckpts/model.pkl\n",
      "Saved meta  → best_ckpts/meta.json\n"
     ]
    }
   ],
   "source": [
    "# save model + metadata for clean reload later\n",
    "joblib.dump(hgb, os.path.join(best_ckpt_dir, \"model.pkl\"))\n",
    "\n",
    "meta = {\n",
    "    \"config\": best_cfg,\n",
    "    \"val_threshold\": float(val_thr),\n",
    "    \"feat_dim\": int(X_tr.shape[1]),\n",
    "    \"seed\": SEED,\n",
    "    \"trained_on\": \"train_only\",\n",
    "    \"metrics\": {\"val_precision\": float(p), \"val_recall\": float(r), \"val_F1\": float(f1)},\n",
    "}\n",
    "with open(os.path.join(best_ckpt_dir, \"meta.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(f\"Saved model → {os.path.join(best_ckpt_dir, 'model.pkl')}\")\n",
    "print(f\"Saved meta  → {os.path.join(best_ckpt_dir, 'meta.json')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa4213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
