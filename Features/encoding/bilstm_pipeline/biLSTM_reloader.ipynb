{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16fcdd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biLSTM import BiLSTMClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "import time\n",
    "import itertools\n",
    "import hashlib\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# PyTorch (CPU & CUDA)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # if multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6898c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9ec1ef",
   "metadata": {},
   "source": [
    "# helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def vocab_to_id_mapper(\n",
    "        input_df\n",
    "        ,max_len\n",
    "        ,sp\n",
    ") :\n",
    "    \n",
    "    pad_id = sp.piece_to_id(\"<pad>\")\n",
    "    if pad_id == -1:  \n",
    "        pad_id = 0\n",
    "\n",
    "    \n",
    "    def encode_ids(text) :\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\" if pd.isna(text) else str(text)\n",
    "        return sp.encode_as_ids(text)\n",
    "\n",
    "    def pad_ids(ids,max_len,pad_id) -> np.ndarray:\n",
    "        if len(ids) >= max_len:\n",
    "            return np.array(ids[:max_len], dtype=np.int32)\n",
    "        return np.array(ids + [pad_id] * (max_len - len(ids)), dtype=np.int32)\n",
    "\n",
    "    \n",
    "    df = input_df.copy()\n",
    "    df[\"sp_ids\"] = df[\"Body\"].apply(encode_ids)\n",
    "\n",
    "    # overwrite sp_ids_padded with NumPy arrays directly\n",
    "    df[\"sp_ids_padded\"] = df[\"sp_ids\"].apply(lambda ids: pad_ids(ids, max_len, pad_id))\n",
    "\n",
    "    return df\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def df_to_ids_and_labels(df):\n",
    "    # 'sp_ids_padded' should be a list/array per row; stack to [N, max_len]\n",
    "    X_ids = np.stack(df[\"sp_ids_padded\"].values).astype(np.int64)\n",
    "    y = df[\"label\"].astype(np.int64).values\n",
    "    return X_ids, y\n",
    "\n",
    "def make_loader(X_ids, y, batch_size=128, shuffle=False):\n",
    "    X = torch.tensor(X_ids, dtype=torch.long)\n",
    "    y = torch.tensor(y,     dtype=torch.long)\n",
    "    ds = TensorDataset(X, y)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, pin_memory=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(encoder, dl, device):\n",
    "    encoder.eval()\n",
    "    feats, labels = [], []\n",
    "    for xb, yb in dl:\n",
    "        xb = xb.to(device)\n",
    "        z  = encoder(xb)                                # [B, feat_dim] (e.g., 512)\n",
    "        feats.append(z.cpu().numpy().astype(np.float32))\n",
    "        labels.append(yb.numpy().astype(np.int64))\n",
    "    return np.concatenate(feats), np.concatenate(labels)\n",
    "\n",
    "\n",
    "class TextDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(np.stack(X), dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39553b92",
   "metadata": {},
   "source": [
    "# base model reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50caebd4",
   "metadata": {},
   "source": [
    "#### test set data tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f69bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dir_path = 'best_ckpts'\n",
    "\n",
    "with open(best_dir_path+'/manifest.json','r') as f:\n",
    "    mf = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "embed_matrix_path = mf['embedding_matrix_file']\n",
    "sp_model_path = mf['sp_model_path']\n",
    "best_ckpt = mf['best_ckpt']\n",
    "\n",
    "pad_id       = int(mf[\"pad_id\"])\n",
    "max_len      = int(mf[\"max_len\"])\n",
    "hidden_dim   = int(mf[\"hidden_dim\"])\n",
    "num_layers   = int(mf[\"num_layers\"])\n",
    "bidirectional= bool(mf[\"bidirectional\"])\n",
    "num_classes  = int(mf[\"num_classes\"])\n",
    "droupout     = float(mf['dropout'])\n",
    "weight_decay = float(mf['weight_decay'])\n",
    "best_thr = float(mf['val_threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ac9ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload embedding matrix and sp process for tokenisation of test set\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(sp_model_path)\n",
    "\n",
    "embedding_matrix = np.load(embed_matrix_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81638e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 300)\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)\n",
    "print(sp.get_piece_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77788abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "test_df = pd.read_csv('clean_data_bilstm/test_clean.csv')\n",
    "test_df.rename(columns = {'text_combined':'Body'},inplace=True)\n",
    "test_df = vocab_to_id_mapper(test_df,256,sp)\n",
    "test_ds  = TextDS(test_df['sp_ids_padded'].values, test_df['label'].values)\n",
    "assert test_df['sp_ids_padded'].apply(len).eq(256).all()\n",
    "test_dl  = DataLoader(test_ds, batch_size=128, shuffle=False,\n",
    "                      num_workers=2, pin_memory=True,\n",
    "                      worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b2ce80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:biLSTM:BiLSTM Encoder initialized | emb_dim=300, hidden_dim=256, layers=2, bidirectional=True, freeze_embeddings=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model reloaded successfully onto device cpu\n",
      "Loaded epoch: 10 | Best F1: 0.9899\n",
      " Model Architecture Summary\n",
      "========================================\n",
      "Model Type      : BiLSTMClassifier\n",
      "Hidden Dim      : 256\n",
      "Num Layers      : 2\n",
      "Bidirectional   : True\n",
      "Dropout         : 0.5\n",
      "Embedding Dim   : 300\n",
      "Vocab Size      : 50000\n",
      "Output Classes  : 2\n",
      "========================================\n",
      "Total Parameters: 17,721,794\n",
      "Trainable Params: 2,721,794\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# ---- rebuild model using manifest config ----\n",
    "model = BiLSTMClassifier(\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    pad_id=pad_id,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=droupout,\n",
    "    bidirectional=bidirectional,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "\n",
    "# ---- load checkpoint ----\n",
    "ckpt = torch.load(best_ckpt, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model reloaded successfully onto device {device}\")\n",
    "print(f\"Loaded epoch: {ckpt.get('epoch', '?')} | Best F1: {ckpt.get('best_val_f1', '?'):.4f}\")\n",
    "print(\" Model Architecture Summary\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Model Type      : {model.__class__.__name__}\")\n",
    "print(f\"Hidden Dim      : {hidden_dim}\")\n",
    "print(f\"Num Layers      : {num_layers}\")\n",
    "print(f\"Bidirectional   : {bidirectional}\")\n",
    "print(f\"Dropout         : {droupout}\")\n",
    "print(f\"Embedding Dim   : {embedding_matrix.shape[1]}\")\n",
    "print(f\"Vocab Size      : {embedding_matrix.shape[0]}\")\n",
    "print(f\"Output Classes  : {num_classes}\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable Params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3e9bf8",
   "metadata": {},
   "source": [
    "# ablation reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3e0d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(best_dir_path+'/meta.json', \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "val_thr   = float(meta[\"val_threshold\"])\n",
    "feat_dim  = int(meta[\"feat_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b989e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:biLSTM:BiLSTM Encoder initialized | emb_dim=300, hidden_dim=256, layers=2, bidirectional=True, freeze_embeddings=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HistGB classifier head loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/javier/miniconda3/envs/dsa4213/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.7.2 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/javier/miniconda3/envs/dsa4213/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator _BinMapper from version 1.7.2 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/javier/miniconda3/envs/dsa4213/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator HistGradientBoostingClassifier from version 1.7.2 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "abl_model = BiLSTMClassifier(\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    pad_id=pad_id,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=droupout,\n",
    "    bidirectional=bidirectional,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "\n",
    "# ---- load checkpoint ----\n",
    "ckpt = torch.load(best_ckpt, map_location=device)\n",
    "abl_model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "abl_model.eval()\n",
    "encoder = abl_model.encoder\n",
    "encoder.eval()\n",
    "\n",
    "\n",
    "hgb = joblib.load(best_dir_path+'/model.pkl')\n",
    "print(\"✅ HistGB classifier head loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d33e8",
   "metadata": {},
   "source": [
    "#### test data tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d64004e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/javier/miniconda3/envs/dsa4213/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# test_df is from upstream\n",
    "Xte_ids, yte = df_to_ids_and_labels(test_df)\n",
    "abl_test_dl  = make_loader(Xte_ids, yte, batch_size=128, shuffle=False)\n",
    "X_te, y_te = extract_features(encoder, test_dl,  device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58f61774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: (8056, 512)\n"
     ]
    }
   ],
   "source": [
    "assert X_te.shape[1] == feat_dim, f\"feat dim mismatch: {X_te.shape[1]} vs meta {feat_dim}\"\n",
    "print(\"Features:\", X_te.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa4213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
