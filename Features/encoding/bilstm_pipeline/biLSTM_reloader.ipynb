{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16fcdd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biLSTM import BiLSTMClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "import time\n",
    "import itertools\n",
    "import hashlib\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# PyTorch (CPU & CUDA)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # if multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6898c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9ec1ef",
   "metadata": {},
   "source": [
    "# helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a4064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def vocab_to_id_mapper(\n",
    "        input_df\n",
    "        ,max_len\n",
    "        ,sp\n",
    ") :\n",
    "    \n",
    "    pad_id = sp.piece_to_id(\"<pad>\")\n",
    "    if pad_id == -1:  \n",
    "        pad_id = 0\n",
    "\n",
    "    \n",
    "    def encode_ids(text) :\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\" if pd.isna(text) else str(text)\n",
    "        return sp.encode_as_ids(text)\n",
    "\n",
    "    def pad_ids(ids,max_len,pad_id) -> np.ndarray:\n",
    "        if len(ids) >= max_len:\n",
    "            return np.array(ids[:max_len], dtype=np.int32)\n",
    "        return np.array(ids + [pad_id] * (max_len - len(ids)), dtype=np.int32)\n",
    "\n",
    "    \n",
    "    df = input_df.copy()\n",
    "    df[\"sp_ids\"] = df[\"Body\"].apply(encode_ids)\n",
    "\n",
    "    # overwrite sp_ids_padded with NumPy arrays directly\n",
    "    df[\"sp_ids_padded\"] = df[\"sp_ids\"].apply(lambda ids: pad_ids(ids, max_len, pad_id))\n",
    "\n",
    "    return df\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def df_to_ids_and_labels(df):\n",
    "    # 'sp_ids_padded' should be a list/array per row; stack to [N, max_len]\n",
    "    X_ids = np.stack(df[\"sp_ids_padded\"].values).astype(np.int64)\n",
    "    y = df[\"label\"].astype(np.int64).values\n",
    "    return X_ids, y\n",
    "\n",
    "def make_loader(X_ids, y, batch_size=128, shuffle=False):\n",
    "    X = torch.tensor(X_ids, dtype=torch.long)\n",
    "    y = torch.tensor(y,     dtype=torch.long)\n",
    "    ds = TensorDataset(X, y)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, pin_memory=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(encoder, dl, device):\n",
    "    encoder.eval()\n",
    "    feats, labels = [], []\n",
    "    for xb, yb in dl:\n",
    "        xb = xb.to(device)\n",
    "        z  = encoder(xb)                                # [B, feat_dim] (e.g., 512)\n",
    "        feats.append(z.cpu().numpy().astype(np.float32))\n",
    "        labels.append(yb.numpy().astype(np.int64))\n",
    "    return np.concatenate(feats), np.concatenate(labels)\n",
    "\n",
    "\n",
    "class TextDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(np.stack(X), dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39553b92",
   "metadata": {},
   "source": [
    "# base model reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50caebd4",
   "metadata": {},
   "source": [
    "#### test set data tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b9f69bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dir_path = 'best_ckpts'\n",
    "\n",
    "with open(best_dir_path+'/manifest.json','r') as f:\n",
    "    mf = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "embed_matrix_path = mf['embedding_matrix_file']\n",
    "sp_model_path = mf['sp_model_path']\n",
    "best_ckpt = mf['best_ckpt']\n",
    "\n",
    "pad_id       = int(mf[\"pad_id\"])\n",
    "max_len      = int(mf[\"max_len\"])\n",
    "hidden_dim   = int(mf[\"hidden_dim\"])\n",
    "num_layers   = int(mf[\"num_layers\"])\n",
    "bidirectional= bool(mf[\"bidirectional\"])\n",
    "num_classes  = int(mf[\"num_classes\"])\n",
    "droupout     = float(mf['dropout'])\n",
    "weight_decay = float(mf['weight_decay'])\n",
    "best_thr = float(mf['val_threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ac9ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload embedding matrix and sp process for tokenisation of test set\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(sp_model_path)\n",
    "\n",
    "embedding_matrix = np.load(embed_matrix_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81638e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 300)\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)\n",
    "print(sp.get_piece_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77788abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "test_df = pd.read_csv('clean_data_bilstm/test_clean.csv')\n",
    "test_df.rename(columns = {'text_combined':'Body'},inplace=True)\n",
    "test_df = vocab_to_id_mapper(test_df,256,sp)\n",
    "test_ds  = TextDS(test_df['sp_ids_padded'].values, test_df['label'].values)\n",
    "assert test_df['sp_ids_padded'].apply(len).eq(256).all()\n",
    "test_dl  = DataLoader(test_ds, batch_size=128, shuffle=False,\n",
    "                      num_workers=2, pin_memory=True,\n",
    "                      worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b2ce80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:biLSTM:BiLSTM Encoder initialized | emb_dim=300, hidden_dim=256, layers=2, bidirectional=True, freeze_embeddings=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model reloaded successfully onto device cpu\n",
      "Loaded epoch: 10 | Best F1: 0.9899\n",
      " Model Architecture Summary\n",
      "========================================\n",
      "Model Type      : BiLSTMClassifier\n",
      "Hidden Dim      : 256\n",
      "Num Layers      : 2\n",
      "Bidirectional   : True\n",
      "Dropout         : 0.5\n",
      "Embedding Dim   : 300\n",
      "Vocab Size      : 50000\n",
      "Output Classes  : 2\n",
      "========================================\n",
      "Total Parameters: 17,721,794\n",
      "Trainable Params: 2,721,794\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# ---- rebuild model using manifest config ----\n",
    "model = BiLSTMClassifier(\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    pad_id=pad_id,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=droupout,\n",
    "    bidirectional=bidirectional,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "\n",
    "# ---- load checkpoint ----\n",
    "ckpt = torch.load(best_ckpt, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model reloaded successfully onto device {device}\")\n",
    "print(f\"Loaded epoch: {ckpt.get('epoch', '?')} | Best F1: {ckpt.get('best_val_f1', '?'):.4f}\")\n",
    "print(\" Model Architecture Summary\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Model Type      : {model.__class__.__name__}\")\n",
    "print(f\"Hidden Dim      : {hidden_dim}\")\n",
    "print(f\"Num Layers      : {num_layers}\")\n",
    "print(f\"Bidirectional   : {bidirectional}\")\n",
    "print(f\"Dropout         : {droupout}\")\n",
    "print(f\"Embedding Dim   : {embedding_matrix.shape[1]}\")\n",
    "print(f\"Vocab Size      : {embedding_matrix.shape[0]}\")\n",
    "print(f\"Output Classes  : {num_classes}\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable Params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3e9bf8",
   "metadata": {},
   "source": [
    "# ablation reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3e0d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(best_dir_path+'/meta.json', \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "val_thr   = float(meta[\"val_threshold\"])\n",
    "feat_dim  = int(meta[\"feat_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b989e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:biLSTM:BiLSTM Encoder initialized | emb_dim=300, hidden_dim=256, layers=2, bidirectional=True, freeze_embeddings=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HistGB classifier head loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/javier/miniconda3/envs/dsa4213/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.7.2 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/javier/miniconda3/envs/dsa4213/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator _BinMapper from version 1.7.2 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/javier/miniconda3/envs/dsa4213/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator HistGradientBoostingClassifier from version 1.7.2 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "abl_model = BiLSTMClassifier(\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    pad_id=pad_id,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=droupout,\n",
    "    bidirectional=bidirectional,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "\n",
    "# ---- load checkpoint ----\n",
    "ckpt = torch.load(best_ckpt, map_location=device)\n",
    "abl_model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "abl_model.eval()\n",
    "encoder = abl_model.encoder\n",
    "encoder.eval()\n",
    "\n",
    "\n",
    "hgb = joblib.load(best_dir_path+'/model.pkl')\n",
    "print(\"✅ HistGB classifier head loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d33e8",
   "metadata": {},
   "source": [
    "#### test data tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d64004e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/javier/miniconda3/envs/dsa4213/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# test_df is from upstream\n",
    "Xte_ids, yte = df_to_ids_and_labels(test_df)\n",
    "abl_test_dl  = make_loader(Xte_ids, yte, batch_size=128, shuffle=False)\n",
    "X_te, y_te = extract_features(encoder, test_dl,  device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58f61774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: (8056, 512)\n"
     ]
    }
   ],
   "source": [
    "assert X_te.shape[1] == feat_dim, f\"feat dim mismatch: {X_te.shape[1]} vs meta {feat_dim}\"\n",
    "print(\"Features:\", X_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020b4ec8",
   "metadata": {},
   "source": [
    "# ablation 2 reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a849fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "cross_dom_df = pd.read_csv('clean_data_bilstm/cross_domain_clean.csv')\n",
    "cross_dom_df.rename(columns = {'cleaned':'Body'},inplace=True)\n",
    "cross_dom_df = vocab_to_id_mapper(cross_dom_df,256,sp)\n",
    "cross_dom_ds  = TextDS(cross_dom_df['sp_ids_padded'].values, cross_dom_df['label'].values)\n",
    "assert cross_dom_df['sp_ids_padded'].apply(len).eq(256).all()\n",
    "cross_dom_dl  = DataLoader(cross_dom_ds, batch_size=128, shuffle=False,\n",
    "                      num_workers=2, pin_memory=True,\n",
    "                      worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "797c6815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>Body</th>\n",
       "      <th>label</th>\n",
       "      <th>sp_ids</th>\n",
       "      <th>sp_ids_padded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[248, 1066, 4713, 525, 765, 6448, 753, 507, 43...</td>\n",
       "      <td>[248, 1066, 4713, 525, 765, 6448, 753, 507, 43...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>0</td>\n",
       "      <td>[1358, 1018, 24452, 20932, 84, 38488]</td>\n",
       "      <td>[1358, 1018, 24452, 20932, 84, 38488, 7, 7, 7,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in a wkly comp to win fa cup final ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[526, 2516, 43, 9, 10718, 113, 203, 30, 1209, ...</td>\n",
       "      <td>[526, 2516, 43, 9, 10718, 113, 203, 30, 1209, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>0</td>\n",
       "      <td>[84, 5081, 1017, 221, 1822, 4510, 84, 16, 1251...</td>\n",
       "      <td>[84, 5081, 1017, 221, 1822, 4510, 84, 16, 1251...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>0</td>\n",
       "      <td>[18302, 24, 619, 554, 204, 3292, 30, 30831, 20...</td>\n",
       "      <td>[18302, 24, 619, 554, 204, 3292, 30, 30831, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088</th>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>this is the 2nd time we have tried contact u u...</td>\n",
       "      <td>1</td>\n",
       "      <td>[85, 68, 18, 876, 728, 289, 70, 116, 3976, 499...</td>\n",
       "      <td>[85, 68, 18, 876, 728, 289, 70, 116, 3976, 499...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5089</th>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>will b going to esplanade fr home</td>\n",
       "      <td>0</td>\n",
       "      <td>[110, 27, 628, 30, 1133, 10541, 669, 115, 937]</td>\n",
       "      <td>[110, 27, 628, 30, 1133, 10541, 669, 115, 937,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>pity was in mood for that soany other suggestions</td>\n",
       "      <td>0</td>\n",
       "      <td>[34617, 250, 43, 7581, 60, 89, 221, 1714, 409,...</td>\n",
       "      <td>[34617, 250, 43, 7581, 60, 89, 221, 1714, 409,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>the guy did some bitching but i acted like id ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[18, 3174, 643, 365, 45703, 284, 24, 14716, 39...</td>\n",
       "      <td>[18, 3174, 643, 365, 45703, 284, 24, 14716, 39...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5092</th>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>rofl its true to its name</td>\n",
       "      <td>0</td>\n",
       "      <td>[291, 3108, 419, 3243, 30, 419, 773]</td>\n",
       "      <td>[291, 3108, 419, 3243, 30, 419, 773, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5093 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          original_text  \\\n",
       "0     Go until jurong point, crazy.. Available only ...   \n",
       "1                         Ok lar... Joking wif u oni...   \n",
       "2     Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3     U dun say so early hor... U c already then say...   \n",
       "4     Nah I don't think he goes to usf, he lives aro...   \n",
       "...                                                 ...   \n",
       "5088  This is the 2nd time we have tried 2 contact u...   \n",
       "5089              Will Ì_ b going to esplanade fr home?   \n",
       "5090  Pity, * was in mood for that. So...any other s...   \n",
       "5091  The guy did some bitching but I acted like i'd...   \n",
       "5092                         Rofl. Its true to its name   \n",
       "\n",
       "                                                   Body  label  \\\n",
       "0     go until jurong point crazy available only in ...      0   \n",
       "1                               ok lar joking wif u oni      0   \n",
       "2     free entry in a wkly comp to win fa cup final ...      1   \n",
       "3           u dun say so early hor u c already then say      0   \n",
       "4     nah i dont think he goes to usf he lives aroun...      0   \n",
       "...                                                 ...    ...   \n",
       "5088  this is the 2nd time we have tried contact u u...      1   \n",
       "5089                  will b going to esplanade fr home      0   \n",
       "5090  pity was in mood for that soany other suggestions      0   \n",
       "5091  the guy did some bitching but i acted like id ...      0   \n",
       "5092                          rofl its true to its name      0   \n",
       "\n",
       "                                                 sp_ids  \\\n",
       "0     [248, 1066, 4713, 525, 765, 6448, 753, 507, 43...   \n",
       "1                 [1358, 1018, 24452, 20932, 84, 38488]   \n",
       "2     [526, 2516, 43, 9, 10718, 113, 203, 30, 1209, ...   \n",
       "3     [84, 5081, 1017, 221, 1822, 4510, 84, 16, 1251...   \n",
       "4     [18302, 24, 619, 554, 204, 3292, 30, 30831, 20...   \n",
       "...                                                 ...   \n",
       "5088  [85, 68, 18, 876, 728, 289, 70, 116, 3976, 499...   \n",
       "5089     [110, 27, 628, 30, 1133, 10541, 669, 115, 937]   \n",
       "5090  [34617, 250, 43, 7581, 60, 89, 221, 1714, 409,...   \n",
       "5091  [18, 3174, 643, 365, 45703, 284, 24, 14716, 39...   \n",
       "5092               [291, 3108, 419, 3243, 30, 419, 773]   \n",
       "\n",
       "                                          sp_ids_padded  \n",
       "0     [248, 1066, 4713, 525, 765, 6448, 753, 507, 43...  \n",
       "1     [1358, 1018, 24452, 20932, 84, 38488, 7, 7, 7,...  \n",
       "2     [526, 2516, 43, 9, 10718, 113, 203, 30, 1209, ...  \n",
       "3     [84, 5081, 1017, 221, 1822, 4510, 84, 16, 1251...  \n",
       "4     [18302, 24, 619, 554, 204, 3292, 30, 30831, 20...  \n",
       "...                                                 ...  \n",
       "5088  [85, 68, 18, 876, 728, 289, 70, 116, 3976, 499...  \n",
       "5089  [110, 27, 628, 30, 1133, 10541, 669, 115, 937,...  \n",
       "5090  [34617, 250, 43, 7581, 60, 89, 221, 1714, 409,...  \n",
       "5091  [18, 3174, 643, 365, 45703, 284, 24, 14716, 39...  \n",
       "5092  [291, 3108, 419, 3243, 30, 419, 773, 7, 7, 7, ...  \n",
       "\n",
       "[5093 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_dom_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa4213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
