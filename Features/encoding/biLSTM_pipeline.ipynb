{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d162a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import biLSTM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e966e61",
   "metadata": {},
   "source": [
    "# Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "390de247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking special token\n",
    "def mask_tokens(text):\n",
    "    # replace URLs (http, https, www)\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '<URL>', text)\n",
    "\n",
    "    # replace common file extensions (customize list)\n",
    "    text = re.sub(r'\\b[\\w\\-]+\\.(pdf|docx|xlsx|txt|csv|tar|doc\\.gz|doc)\\b', '<FILE>', text)\n",
    "\n",
    "    # emails\n",
    "    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
    "\n",
    "    # money \n",
    "    text = re.sub(r'\\$\\d+(?:\\.\\d{2})?','<MONEY>',text)\n",
    "\n",
    "    # numbers \n",
    "    text = re.sub(r'\\b\\d+\\b','<NUMBER>',text)\n",
    "    text = text.replace('<NUMBER>', '')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# un HTML raw text \n",
    "def strip_html(raw_html):\n",
    "    \"\"\"\n",
    "    Strip HTML tags, scripts, styles, and normalize whitespace\n",
    "    to return clean raw text from HTML emails.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "\n",
    "    \n",
    "    for a in soup.find_all(\"a\"):\n",
    "        href = a.get(\"href\",None)\n",
    "        if not href:   # skip if no href\n",
    "                continue\n",
    "\n",
    "        # print(a_attribute)\n",
    "\n",
    "        a_attribute = mask_tokens(href)\n",
    "\n",
    "        if a_attribute == '<URL>' : \n",
    "            a.replace_with('<URL>')\n",
    "\n",
    "        elif a_attribute =='<EMAIL>' : \n",
    "            a.replace_with('<EMAIL>')\n",
    "        \n",
    "        elif a_attribute == '<FILE>' : \n",
    "            a.replace_with('<FILE>')\n",
    "\n",
    "        elif a_attribute == '<MONEY>' : \n",
    "            a.replace_with('<MONEY>')\n",
    "        \n",
    "        elif a_attribute == '<NUMBER>' : \n",
    "            a.replace_with('<NUMBER>')\n",
    "\n",
    "    # remove script, style, head, and metadata tags\n",
    "    for tag in soup([\"script\", \"style\", \"head\", \"title\", \"meta\", \"[document]\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # extract text\n",
    "    text = soup.get_text(separator=\" \")\n",
    "\n",
    "    # normalize unicode \n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # replace non-breaking spaces specifically (unicode)\n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "\n",
    "    # collapse all whitespace tokens (line breaks, tabs, multiple spaces) into one space and remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # rim leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# special case handling\n",
    "mapper = str.maketrans({\n",
    "    '0':'o','1':'l','3':'e','4':'a','5':'s','7':'t','$':'s','@':'a'\n",
    "})\n",
    "\n",
    "def deobfuscate_words(text):\n",
    "    \"\"\"\n",
    "    capture non-alphanumeric sequence in windows of 1-3 and replaces with ' ' \n",
    "    l-o-v-e -> l-o , - is detected and removed -> love\n",
    "    \"\"\"\n",
    "    # replace text to number \n",
    "    text = text.translate(mapper)\n",
    "    # remove weird spaces etc \n",
    "    text = re.sub(r'(?i)(?<=\\w)[^A-Za-z0-9\\s]{1,3}(?=\\w)', '', text)\n",
    "    return text\n",
    "\n",
    "def word_capper(text):\n",
    "    text = re.sub(r'(.)\\1{' + str(2) + r',}', lambda m: m.group(1)*2, text)\n",
    "    text = re.sub(r'([!?.,])\\1{1,}', r'\\1\\1', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# whitelist filtering\n",
    "def char_lvl_whitelist_filter(text): \n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\'\\\":;\\-\\_\\(\\)\\@\\#\\$\\%\\^\\&\\<\\>]', '', text)\n",
    "    return text\n",
    "\n",
    "# word level processor \n",
    "def lemmatizer(text) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = ''\n",
    "\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    return sentence.join(lemmatized_words)\n",
    "\n",
    "#final clean\n",
    "def final_punc_removal(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s<>]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "    \n",
    "def preprocess_email_text(raw): \n",
    "    \"\"\"\n",
    "    the whole pipeline of processing\n",
    "    input : dataframe with text column and ham/spam label\n",
    "    output : dataframe with cleaned sentences and ham/spam label\n",
    "    \"\"\"\n",
    "    raw = strip_html(raw) # process html first to capture links from <a> tags\n",
    "    raw = mask_tokens(raw) # mask special tokens \n",
    "    raw = deobfuscate_words(raw)\n",
    "    raw = word_capper(raw)\n",
    "    raw = lemmatizer(raw)\n",
    "    raw = char_lvl_whitelist_filter(raw)\n",
    "    raw = final_punc_removal(raw)\n",
    "    raw = raw.lower()\n",
    "    return raw\n",
    "\n",
    "def preprocess_email_df(df, text_col):\n",
    "    df[text_col] = df[text_col].apply(preprocess_email_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "def vocab_builder(\n",
    "    input_df\n",
    "    ,vocab_size\n",
    "    ,model_type\n",
    ") : \n",
    "    \n",
    "    input_df[\"Body\"].to_csv(\"emails_clean.txt\", index=False, header=False)\n",
    "\n",
    "    # train SentencePiece model\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f\"--input=emails_clean.txt \"\n",
    "        f\"--model_prefix=email_sp \"\n",
    "        f\"--vocab_size={vocab_size} \"\n",
    "        f\"--character_coverage=1.0 \"\n",
    "        f\"--model_type={model_type} \"\n",
    "        f\"--shuffle_input_sentence=false \"\n",
    "        f\"--seed_sentencepiece_size=1000000 \"\n",
    "        f\"--user_defined_symbols=<url>,<email>,<file>,<money>,<pad>\"\n",
    "    )\n",
    "\n",
    "\n",
    "def vocab_to_id_mapper(\n",
    "        input_df\n",
    "        ,max_len\n",
    ") :\n",
    "    \n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(\"email_sp.model\")                 \n",
    "\n",
    "\n",
    "    \n",
    "    MAX_LEN = max_len\n",
    "    pad_id = sp.piece_to_id(\"<pad>\")\n",
    "    if pad_id == -1:  \n",
    "        pad_id = 0\n",
    "\n",
    "    \n",
    "    def encode_ids(text) :\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\" if pd.isna(text) else str(text)\n",
    "        return sp.encode_as_ids(text)\n",
    "\n",
    "    def pad_ids(ids,max_len,pad_id) -> np.ndarray:\n",
    "        if len(ids) >= max_len:\n",
    "            return np.array(ids[:max_len], dtype=np.int32)\n",
    "        return np.array(ids + [pad_id] * (max_len - len(ids)), dtype=np.int32)\n",
    "\n",
    "    \n",
    "    df = input_df.copy()\n",
    "    df[\"sp_ids\"] = df[\"Body\"].apply(encode_ids)\n",
    "\n",
    "    # overwrite sp_ids_padded with NumPy arrays directly\n",
    "    df[\"sp_ids_padded\"] = df[\"sp_ids\"].apply(lambda ids: pad_ids(ids, MAX_LEN, pad_id))\n",
    "\n",
    "    return df\n",
    "####################################\n",
    "\n",
    "\n",
    "def vocab_to_id_mapper(\n",
    "        input_df\n",
    "        ,max_len\n",
    "        ,sp\n",
    ") :\n",
    "    \n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "    MAX_LEN = max_len\n",
    "    pad_id = sp.piece_to_id(\"<pad>\")\n",
    "    if pad_id == -1:  \n",
    "        pad_id = 0\n",
    "\n",
    "    \n",
    "    def encode_ids(text) :\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\" if pd.isna(text) else str(text)\n",
    "        return sp.encode_as_ids(text)\n",
    "\n",
    "    def pad_ids(ids,max_len,pad_id) -> np.ndarray:\n",
    "        if len(ids) >= max_len:\n",
    "            return np.array(ids[:max_len], dtype=np.int32)\n",
    "        return np.array(ids + [pad_id] * (max_len - len(ids)), dtype=np.int32)\n",
    "\n",
    "    \n",
    "    df = input_df.copy()\n",
    "    df[\"sp_ids\"] = df[\"Body\"].apply(encode_ids)\n",
    "\n",
    "    # overwrite sp_ids_padded with NumPy arrays directly\n",
    "    df[\"sp_ids_padded\"] = df[\"sp_ids\"].apply(lambda ids: pad_ids(ids, MAX_LEN, pad_id))\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_embedding_matrix(w2v, sp, pad_id: int, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Build embedding matrix aligned with SentencePiece IDs.\n",
    "    \"\"\"\n",
    "    vocab_size = sp.get_piece_size()\n",
    "    emb_dim = w2v.vector_size\n",
    "\n",
    "    E = np.zeros((vocab_size, emb_dim), dtype=np.float32)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for sp_id in range(vocab_size):\n",
    "        piece = sp.id_to_piece(sp_id)\n",
    "        if piece in w2v.wv:\n",
    "            E[sp_id] = w2v.wv[piece]\n",
    "        else:\n",
    "            E[sp_id] = rng.normal(0.0, 0.01, size=emb_dim).astype(np.float32)\n",
    "\n",
    "    # Keep PAD = 0\n",
    "    if 0 <= pad_id < vocab_size:\n",
    "        E[pad_id] = 0.0\n",
    "\n",
    "    metadata = {\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"pad_id\": pad_id,\n",
    "        \"trained_vocab\": len(w2v.wv),\n",
    "        \"oov_count\": vocab_size - len(w2v.wv),\n",
    "    }\n",
    "    return E, metadata\n",
    "\n",
    "class TextDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(np.stack(X), dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c816aa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Word2Vec object from full_447_batch_A/word2vec.model\n",
      "INFO:gensim.utils:loading wv recursively from full_447_batch_A/word2vec.model.wv.* with mmap=None\n",
      "INFO:gensim.utils:loading vectors from full_447_batch_A/word2vec.model.wv.vectors.npy with mmap=None\n",
      "INFO:gensim.utils:loading syn1neg from full_447_batch_A/word2vec.model.syn1neg.npy with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname': 'full_447_batch_A/word2vec.model', 'datetime': '2025-10-11T23:32:22.096594', 'gensim': '4.3.3', 'python': '3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 08:03:38) [Clang 14.0.6 ]', 'platform': 'macOS-14.5-arm64-arm-64bit', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "# build sp -> word embedin matrix \n",
    "load_path = 'full_447_batch_A/'\n",
    "\n",
    "\n",
    "# load saved model\n",
    "w2v_model = Word2Vec.load(load_path+\"word2vec.model\")\n",
    "\n",
    "#sentencePiece model & pad_id\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(load_path+\"email_sp.model\")\n",
    "pad_id = sp.piece_to_id(\"<pad>\")\n",
    "if pad_id == -1:\n",
    "    pad_id = 0\n",
    "\n",
    "subword_processor = sp \n",
    "\n",
    "embedding_matrix, embedding_summary = build_embedding_matrix(w2v_model,subword_processor,pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5987ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50000,\n",
       " 'emb_dim': 300,\n",
       " 'pad_id': 7,\n",
       " 'trained_vocab': 48242,\n",
       " 'oov_count': 1758}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "394adcef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_id = subword_processor.piece_to_id('<pad>')\n",
    "pad_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414af049",
   "metadata": {},
   "source": [
    "#### Train-valid-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254dce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_raw = pd.read_csv('raw_encoder_data_sets/train_set.csv')\n",
    "val_df_raw = pd.read_csv('raw_encoder_data_sets/train_set.csv')\n",
    "test_df_raw = pd.read_csv('raw_encoder_data_sets/train_set.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46b6270e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text_combined    object\n",
       "label             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4824eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess same as word2vec preprocessing\n",
    "train_df = preprocess_email_df(train_df_raw,'text_combined')\n",
    "val_df = preprocess_email_df(val_df_raw,'text_combined')\n",
    "test_df = preprocess_email_df(test_df_raw,'text_combined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6806c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise and pad\n",
    "train_df = vocab_to_id_mapper(train_df,256,sp)\n",
    "val_df = vocab_to_id_mapper(val_df,256,sp)\n",
    "test_df = vocab_to_id_mapper(test_df,256,sp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0bda2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch object for model injection\n",
    "\n",
    "train_ds = TextDS(train_df['sp_ids_padded'].values, train_df['label'].values)\n",
    "val_ds   = TextDS(val_df['sp_ids_padded'].values, val_df['label'].values)\n",
    "test_ds  = TextDS(test_df['sp_ids_padded'].values, test_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ad8d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_df['sp_ids_padded'].apply(len).eq(256).all()\n",
    "assert val_df['sp_ids_padded'].apply(len).eq(256).all()\n",
    "assert test_df['sp_ids_padded'].apply(len).eq(256).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1855f543",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd843bf4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa4213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
