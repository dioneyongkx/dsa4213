{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f146965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/dsa4213/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_recall_fscore_support, \n",
    "    classification_report, accuracy_score\n",
    ")\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from peft import PeftModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c3cd31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt_dir = 'best_ckpts_distilbert'\n",
    "\n",
    "def make_hgb(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    max_iter=600,\n",
    "    l2_regularization=1.0,\n",
    "    min_samples_leaf=50,\n",
    "    class_weight=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create HistGradientBoostingClassifier with specified parameters\n",
    "    \"\"\"\n",
    "    return HistGradientBoostingClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        max_iter=max_iter,\n",
    "        l2_regularization=l2_regularization,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        class_weight=class_weight,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=30,\n",
    "        validation_fraction=0.1,\n",
    "        random_state=SEED,\n",
    "        scoring=\"loss\",\n",
    "        categorical_features=None,\n",
    "        monotonic_cst=None\n",
    "    )\n",
    "\n",
    "def f1_with_best_threshold(y_true, proba, average=\"binary\"):\n",
    "    \"\"\"\n",
    "    Sweep thresholds on the validation set to pick the best F1.\n",
    "    Returns (best_f1, best_threshold).\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0.05, 0.95, 19)\n",
    "    best_f1, best_t = -1.0, 0.5\n",
    "    for t in thresholds:\n",
    "        y_pred = (proba >= t).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    return best_f1, best_t\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(encoder, dl, device):\n",
    "    \"\"\"Extract features from DistilBERT encoder\"\"\"\n",
    "    encoder.eval()\n",
    "    feats, labels = [], []\n",
    "    for batch in dl:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        # Extract [CLS] token embeddings\n",
    "        outputs = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        z = outputs.last_hidden_state[:, 0, :]  # [B, 768]\n",
    "        \n",
    "        feats.append(z.cpu().numpy().astype(np.float32))\n",
    "        labels.append(batch['label'].numpy().astype(np.int64))\n",
    "    \n",
    "    return np.concatenate(feats), np.concatenate(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0167f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for email data with DistilBERT tokenization\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def make_loader(dataset, batch_size=128, shuffle=False):\n",
    "    \"\"\"Create DataLoader from dataset\"\"\"\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, \n",
    "                     pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51f2bcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded manifest from: best_ckpts_distilbert/manifest.json\n",
      "LoRA config: r=8, alpha=16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoder reloaded and frozen.\n"
     ]
    }
   ],
   "source": [
    "# Load manifest\n",
    "MANIFEST_PATH = os.path.join(best_ckpt_dir, \"manifest.json\")\n",
    "\n",
    "with open(MANIFEST_PATH, \"r\") as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "print(f\"Loaded manifest from: {MANIFEST_PATH}\")\n",
    "print(f\"LoRA config: r={manifest['lora_r']}, alpha={manifest['lora_alpha']}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load base DistilBERT model\n",
    "base_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load LoRA weights\n",
    "lora_weights_path = manifest[\"lora_weights_path\"]\n",
    "encoder = PeftModel.from_pretrained(base_model, lora_weights_path).to(device)\n",
    "\n",
    "# Freeze encoder\n",
    "encoder.eval()\n",
    "for p in encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"✓ Encoder reloaded and frozen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e973d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap checks:\n",
      "Train-Test: 0\n",
      "Val-Test: 0\n",
      "Train-Val: 0\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../../../datasets/encoder_dataset/clean/distilbert/train_clean.csv')\n",
    "val_df = pd.read_csv('../../../datasets/encoder_dataset/clean/distilbert/val_clean.csv')\n",
    "test_df = pd.read_csv('../../../datasets/encoder_dataset/clean/distilbert/test_clean.csv')\n",
    "\n",
    "\n",
    "text_col = 'text_combined'\n",
    "\n",
    "# Verify no overlaps\n",
    "print(\"Overlap checks:\")\n",
    "print(f\"Train-Test: {len(set(train_df[text_col]) & set(test_df[text_col]))}\")\n",
    "print(f\"Val-Test: {len(set(val_df[text_col]) & set(test_df[text_col]))}\")\n",
    "print(f\"Train-Val: {len(set(train_df[text_col]) & set(val_df[text_col]))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd3cf4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created:\n",
      "  Train batches: 513\n",
      "  Val batches: 63\n",
      "  Test batches: 63\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_ds = EmailDataset(train_df[text_col].values, train_df['label'].values, \n",
    "                        tokenizer, max_length=256)\n",
    "val_ds = EmailDataset(val_df[text_col].values, val_df['label'].values, \n",
    "                      tokenizer, max_length=256)\n",
    "test_ds = EmailDataset(test_df[text_col].values, test_df['label'].values, \n",
    "                       tokenizer, max_length=256)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dl = make_loader(train_ds, batch_size=128, shuffle=False)\n",
    "val_dl = make_loader(val_ds, batch_size=128, shuffle=False)\n",
    "test_dl = make_loader(test_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"DataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_dl)}\")\n",
    "print(f\"  Val batches: {len(val_dl)}\")\n",
    "print(f\"  Test batches: {len(test_dl)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c5646f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: 65655\n",
      "Val shape: 8060\n",
      "Test shape: 8056\n",
      "Train labels: [31385 34270]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train shape: {len(train_df)}\")\n",
    "print(f\"Val shape: {len(val_df)}\")\n",
    "print(f\"Test shape: {len(test_df)}\")\n",
    "print(f\"Train labels: {np.bincount(train_df['label'].values)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c48f207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features from DistilBERT encoder...\n",
      "\n",
      "Feature extraction complete:\n",
      "  Train features: (65655, 768)\n",
      "  Val features: (8060, 768)\n",
      "  Test features: (8056, 768)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExtracting features from DistilBERT encoder...\")\n",
    "\n",
    "X_tr, y_tr = extract_features(encoder, train_dl, device)\n",
    "X_va, y_va = extract_features(encoder, val_dl, device)\n",
    "X_te, y_te = extract_features(encoder, test_dl, device)\n",
    "\n",
    "print(f\"\\nFeature extraction complete:\")\n",
    "print(f\"  Train features: {X_tr.shape}\")\n",
    "print(f\"  Val features: {X_va.shape}\")\n",
    "print(f\"  Test features: {X_te.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "476d72d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search (HGB): 100%|██████████████████████████████████████████████| 8/8 [03:18<00:00, 24.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Best (val): {'learning_rate': 0.03, 'max_depth': 4, 'l2_regularization': 0.0, 'min_samples_leaf': 20, 'max_iter': 600} | F1=0.9937 | thr=0.950\n",
      "\n",
      "✓ Tuning log saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tiny tuning grid (8 configs total)\n",
    "param_grid = [\n",
    "    {\"learning_rate\": 0.03, \"max_depth\": 4, \"l2_regularization\": 0.0, \"min_samples_leaf\": 20, \"max_iter\": 600},\n",
    "    {\"learning_rate\": 0.03, \"max_depth\": 4, \"l2_regularization\": 1.0, \"min_samples_leaf\": 50, \"max_iter\": 600},\n",
    "    {\"learning_rate\": 0.03, \"max_depth\": 6, \"l2_regularization\": 0.0, \"min_samples_leaf\": 20, \"max_iter\": 600},\n",
    "    {\"learning_rate\": 0.03, \"max_depth\": 6, \"l2_regularization\": 1.0, \"min_samples_leaf\": 50, \"max_iter\": 600},\n",
    "\n",
    "    {\"learning_rate\": 0.05, \"max_depth\": 4, \"l2_regularization\": 0.0, \"min_samples_leaf\": 20, \"max_iter\": 600},\n",
    "    {\"learning_rate\": 0.05, \"max_depth\": 4, \"l2_regularization\": 1.0, \"min_samples_leaf\": 50, \"max_iter\": 600},\n",
    "    {\"learning_rate\": 0.05, \"max_depth\": 6, \"l2_regularization\": 0.0, \"min_samples_leaf\": 20, \"max_iter\": 600},\n",
    "    {\"learning_rate\": 0.05, \"max_depth\": 6, \"l2_regularization\": 1.0, \"min_samples_leaf\": 50, \"max_iter\": 600},\n",
    "]\n",
    "\n",
    "best_cfg, best_model, best_val_f1, best_thr = None, None, -1.0, 0.5\n",
    "tuning_log = []\n",
    "\n",
    "for cfg in tqdm(param_grid, desc=\"Grid Search (HGB)\", ncols=100):\n",
    "    hgb = HistGradientBoostingClassifier(\n",
    "        **cfg,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=20,\n",
    "        validation_fraction=0.05,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    hgb.fit(X_tr, y_tr)\n",
    "    proba_va = hgb.predict_proba(X_va)[:, 1]\n",
    "    f1_va, thr = f1_with_best_threshold(y_va, proba_va)\n",
    "\n",
    "    tuning_log.append({**cfg, \"val_F1\": f1_va, \"thr\": thr})\n",
    "\n",
    "    if f1_va > best_val_f1:\n",
    "        best_cfg, best_model, best_val_f1, best_thr = cfg, hgb, f1_va, thr\n",
    "\n",
    "print(f\"✓ Best (val): {best_cfg} | F1={best_val_f1:.4f} | thr={best_thr:.3f}\")\n",
    "\n",
    "# Save tuning log\n",
    "tuning_df = pd.DataFrame(tuning_log).sort_values(\"val_F1\", ascending=False)\n",
    "tuning_df.to_csv(os.path.join(best_ckpt_dir, \"hgb_tuning_log.csv\"), index=False)\n",
    "print(f\"\\n✓ Tuning log saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ff1614",
   "metadata": {},
   "source": [
    "## train best HGB configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b8ac974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] F1=0.9936  P=0.9934  R=0.9937  (thr=0.950)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9929    0.9926    0.9928      3794\n",
      "           1     0.9934    0.9937    0.9936      4266\n",
      "\n",
      "    accuracy                         0.9932      8060\n",
      "   macro avg     0.9932    0.9931    0.9932      8060\n",
      "weighted avg     0.9932    0.9932    0.9932      8060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train on TRAIN ONLY, using the chosen best config\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    **best_cfg,\n",
    "    early_stopping=True, \n",
    "    n_iter_no_change=30, \n",
    "    validation_fraction=0.1,\n",
    "    random_state=SEED\n",
    ")\n",
    "hgb.fit(X_tr, y_tr)\n",
    "\n",
    "# Evaluate on validation\n",
    "proba_va = hgb.predict_proba(X_va)[:, 1]\n",
    "val_f1, val_thr = f1_with_best_threshold(y_va, proba_va)\n",
    "y_hat_va = (proba_va >= val_thr).astype(int)\n",
    "p, r, f1, _ = precision_recall_fscore_support(y_va, y_hat_va, average=\"binary\", zero_division=0)\n",
    "\n",
    "print(f\"[VAL] F1={f1:.4f}  P={p:.4f}  R={r:.4f}  (thr={val_thr:.3f})\")\n",
    "print(classification_report(y_va, y_hat_va, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eb6c5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model → best_ckpts_distilbert/hgb_model.pkl\n",
      "Saved meta  → best_ckpts_distilbert/hgb_meta.json\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "joblib.dump(hgb, os.path.join(best_ckpt_dir, \"hgb_model.pkl\"))\n",
    "\n",
    "# Save metadata\n",
    "meta = {\n",
    "    \"config\": best_cfg,\n",
    "    \"val_threshold\": float(val_thr),\n",
    "    \"feat_dim\": int(X_tr.shape[1]),\n",
    "    \"seed\": SEED,\n",
    "    \"trained_on\": \"train_only\",\n",
    "    \"metrics\": {\n",
    "        \"val_precision\": float(p), \n",
    "        \"val_recall\": float(r), \n",
    "        \"val_F1\": float(f1)\n",
    "    },\n",
    "}\n",
    "with open(os.path.join(best_ckpt_dir, \"hgb_meta.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(f\"Saved model → {os.path.join(best_ckpt_dir, 'hgb_model.pkl')}\")\n",
    "print(f\"Saved meta  → {os.path.join(best_ckpt_dir, 'hgb_meta.json')}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa4213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
