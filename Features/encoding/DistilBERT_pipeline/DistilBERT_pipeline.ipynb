{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9515f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/dsa4213/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc, \n",
    "    roc_auc_score, precision_recall_fscore_support\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import joblib\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method('fork', force=True)\n",
    "\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "\n",
    "from email_dataset import EmailDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c855ea",
   "metadata": {},
   "source": [
    "## Data loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95fc35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking special tokens\n",
    "def mask_tokens(text):\n",
    "    \"\"\"Mask URLs, files, emails, money, and numbers\"\"\"\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', '<URL>', text)\n",
    "    text = re.sub(r'\\b[\\w\\-]+\\.(pdf|docx|xlsx|txt|csv|tar|doc\\.gz|doc)\\b', '<FILE>', text)\n",
    "    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
    "    text = re.sub(r'\\$\\d+(?:\\.\\d{2})?', '<MONEY>', text)\n",
    "    text = re.sub(r'\\b\\d+\\b', '<NUMBER>', text)\n",
    "    text = text.replace('<NUMBER>', '')\n",
    "    return text\n",
    "\n",
    "# Strip HTML\n",
    "def strip_html(raw_html):\n",
    "    \"\"\"Strip HTML tags and extract clean text\"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    \n",
    "    for a in soup.find_all(\"a\"):\n",
    "        href = a.get(\"href\", None)\n",
    "        if not href:\n",
    "            continue\n",
    "        a_attribute = mask_tokens(href)\n",
    "        if a_attribute == '<URL>':\n",
    "            a.replace_with('<URL>')\n",
    "        elif a_attribute == '<EMAIL>':\n",
    "            a.replace_with('<EMAIL>')\n",
    "        elif a_attribute == '<FILE>':\n",
    "            a.replace_with('<FILE>')\n",
    "        elif a_attribute == '<MONEY>':\n",
    "            a.replace_with('<MONEY>')\n",
    "    \n",
    "    for tag in soup([\"script\", \"style\", \"head\", \"title\", \"meta\", \"[document]\"]):\n",
    "        tag.decompose()\n",
    "    \n",
    "    text = soup.get_text(separator=\" \")\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Deobfuscation\n",
    "mapper = str.maketrans({\n",
    "    '0':'o', '1':'l', '3':'e', '4':'a', '5':'s', '7':'t', '$':'s', '@':'a'\n",
    "})\n",
    "\n",
    "def deobfuscate_words(text):\n",
    "    \"\"\"Replace leetspeak and remove weird spacing\"\"\"\n",
    "    text = text.translate(mapper)\n",
    "    text = re.sub(r'(?i)(?<=\\w)[^A-Za-z0-9\\s]{1,3}(?=\\w)', '', text)\n",
    "    return text\n",
    "\n",
    "def word_capper(text):\n",
    "    \"\"\"Cap repeated characters\"\"\"\n",
    "    text = re.sub(r'(.)\\1{2,}', lambda m: m.group(1)*2, text)\n",
    "    text = re.sub(r'([!?.,])\\1{1,}', r'\\1\\1', text)\n",
    "    return text\n",
    "\n",
    "def char_lvl_whitelist_filter(text):\n",
    "    \"\"\"Whitelist filtering\"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\'\\\":;\\-\\_\\(\\)\\@\\#\\$\\%\\^\\&\\<\\>]', '', text)\n",
    "    return text\n",
    "\n",
    "def final_punc_removal(text):\n",
    "    \"\"\"Final punctuation cleanup\"\"\"\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s<>]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_email_text(raw):\n",
    "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "    if pd.isna(raw) or raw == '':\n",
    "        return \"\"\n",
    "    raw = str(raw)\n",
    "    raw = strip_html(raw)\n",
    "    raw = mask_tokens(raw)\n",
    "    raw = deobfuscate_words(raw)\n",
    "    raw = word_capper(raw)\n",
    "    raw = char_lvl_whitelist_filter(raw)\n",
    "    raw = final_punc_removal(raw)\n",
    "    raw = raw.lower()\n",
    "    if len(raw) < 10:\n",
    "        return \"\"\n",
    "    return raw\n",
    "\n",
    "def preprocess_email_df(df, text_col):\n",
    "    \"\"\"Apply preprocessing to entire dataframe\"\"\"\n",
    "    df[text_col] = df[text_col].apply(preprocess_email_text)\n",
    "    # Remove empty texts\n",
    "    df = df[df[text_col] != \"\"].reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269797df",
   "metadata": {},
   "source": [
    "## Create Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cea45ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EmailDataset(Dataset):\n",
    "#     \"\"\"Custom PyTorch Dataset for email data\"\"\"\n",
    "    \n",
    "#     def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         text = str(self.texts[idx])\n",
    "#         label = int(self.labels[idx])\n",
    "        \n",
    "#         encoding = self.tokenizer(\n",
    "#             text,\n",
    "#             padding='max_length',\n",
    "#             truncation=True,\n",
    "#             max_length=self.max_length,\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "        \n",
    "#         return {\n",
    "#             'input_ids': encoding['input_ids'].flatten(),\n",
    "#             'attention_mask': encoding['attention_mask'].flatten(),\n",
    "#             'label': torch.tensor(label, dtype=torch.long)\n",
    "#         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbcb1e2",
   "metadata": {},
   "source": [
    "## Run distilbert with LoRA Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72a4756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTLoRAEncoder(nn.Module):\n",
    "    \"\"\"DistilBERT-based encoder with LoRA for parameter-efficient fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, lora_config=None, use_lora=True):\n",
    "        super(DistilBERTLoRAEncoder, self).__init__()\n",
    "        \n",
    "        # Load base DistilBERT model\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.use_lora = use_lora\n",
    "        \n",
    "        if use_lora:\n",
    "            # Default LoRA configuration if not provided\n",
    "            if lora_config is None:\n",
    "                lora_config = LoraConfig(\n",
    "                    r=8,\n",
    "                    lora_alpha=16,\n",
    "                    target_modules=[\"q_lin\", \"v_lin\"],\n",
    "                    lora_dropout=0.1,\n",
    "                    bias=\"none\",\n",
    "                    task_type=TaskType.FEATURE_EXTRACTION\n",
    "                )\n",
    "            \n",
    "            # Apply LoRA to the model\n",
    "            self.distilbert = get_peft_model(self.distilbert, lora_config)\n",
    "            self.print_trainable_parameters()\n",
    "        else:\n",
    "            # Freeze all parameters if not using LoRA\n",
    "            for param in self.distilbert.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def print_trainable_parameters(self):\n",
    "        \"\"\"Print the number of trainable parameters\"\"\"\n",
    "        if self.use_lora:\n",
    "            trainable_params = 0\n",
    "            all_param = 0\n",
    "            for _, param in self.distilbert.named_parameters():\n",
    "                all_param += param.numel()\n",
    "                if param.requires_grad:\n",
    "                    trainable_params += param.numel()\n",
    "            \n",
    "            print(f\"\\nLoRA Parameter Efficiency:\")\n",
    "            print(f\"  Trainable params: {trainable_params:,}\")\n",
    "            print(f\"  All params: {all_param:,}\")\n",
    "            print(f\"  Trainable%: {100 * trainable_params / all_param:.2f}%\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"Extract hidden states from DistilBERT with LoRA\"\"\"\n",
    "        outputs = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        # Use [CLS] token representation (first token)\n",
    "        hidden_states = outputs.last_hidden_state[:, 0, :]\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9526b77d",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a02b45c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shapes:\n",
      "Train: (65662, 2)\n",
      "Val: (8208, 2)\n",
      "Test: (8208, 2)\n",
      "\n",
      "Cleaned data shapes:\n",
      "Train: (65655, 2)\n",
      "Val: (8060, 2)\n",
      "Test: (8056, 2)\n",
      "\n",
      "Overlap checks (should all be 0):\n",
      "Train-Val: 0\n",
      "Train-Test: 0\n",
      "Val-Test: 0\n"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "train_df_raw = pd.read_csv('../raw_encoder_data_sets/train_set.csv')\n",
    "val_df_raw = pd.read_csv('../raw_encoder_data_sets/valid_set.csv')\n",
    "test_df_raw = pd.read_csv('../raw_encoder_data_sets/test_set.csv')\n",
    "\n",
    "print(\"Raw data shapes:\")\n",
    "print(f\"Train: {train_df_raw.shape}\")\n",
    "print(f\"Val: {val_df_raw.shape}\")\n",
    "print(f\"Test: {test_df_raw.shape}\")\n",
    "\n",
    "# Preprocess\n",
    "text_col = \"text_combined\"\n",
    "train_df = preprocess_email_df(train_df_raw.copy(), text_col)\n",
    "val_df = preprocess_email_df(val_df_raw.copy(), text_col)\n",
    "test_df = preprocess_email_df(test_df_raw.copy(), text_col)\n",
    "\n",
    "# Remove overlaps (Train → Val → Test hierarchy)\n",
    "train_texts = set(train_df[text_col])\n",
    "val_df = val_df[~val_df[text_col].isin(train_texts)].reset_index(drop=True)\n",
    "val_texts = set(val_df[text_col])\n",
    "test_df = test_df[~test_df[text_col].isin(train_texts.union(val_texts))].reset_index(drop=True)\n",
    "\n",
    "print(\"\\nCleaned data shapes:\")\n",
    "print(f\"Train: {train_df.shape}\")\n",
    "print(f\"Val: {val_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n",
    "\n",
    "# Verify no overlaps\n",
    "print(f\"\\nOverlap checks (should all be 0):\")\n",
    "print(f\"Train-Val: {len(set(train_df[text_col]) & set(val_df[text_col]))}\")\n",
    "print(f\"Train-Test: {len(set(train_df[text_col]) & set(test_df[text_col]))}\")\n",
    "print(f\"Val-Test: {len(set(val_df[text_col]) & set(test_df[text_col]))}\")\n",
    "\n",
    "# Save cleaned data\n",
    "os.makedirs('clean_data_distilbert', exist_ok=True)\n",
    "train_df.to_csv('clean_data_distilbert/train_clean.csv', index=False)\n",
    "val_df.to_csv('clean_data_distilbert/val_clean.csv', index=False)\n",
    "test_df.to_csv('clean_data_distilbert/test_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677511e8",
   "metadata": {},
   "source": [
    "## DistilBERT + LoRA Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f81f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTLoRAEncoder(nn.Module):\n",
    "    \"\"\"DistilBERT encoder with LoRA adaptation\"\"\"\n",
    "    \n",
    "    def __init__(self, lora_config=None, use_lora=True):\n",
    "        super(DistilBERTLoRAEncoder, self).__init__()\n",
    "        \n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.use_lora = use_lora\n",
    "        \n",
    "        if use_lora:\n",
    "            if lora_config is None:\n",
    "                lora_config = LoraConfig(\n",
    "                    r=8,\n",
    "                    lora_alpha=16,\n",
    "                    target_modules=[\"q_lin\", \"v_lin\"],\n",
    "                    lora_dropout=0.1,\n",
    "                    bias=\"none\",\n",
    "                    task_type=TaskType.FEATURE_EXTRACTION\n",
    "                )\n",
    "            \n",
    "            self.distilbert = get_peft_model(self.distilbert, lora_config)\n",
    "            self.print_trainable_parameters()\n",
    "        else:\n",
    "            for param in self.distilbert.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def print_trainable_parameters(self):\n",
    "        \"\"\"Print trainable parameter statistics\"\"\"\n",
    "        if self.use_lora:\n",
    "            trainable_params = 0\n",
    "            all_param = 0\n",
    "            for _, param in self.distilbert.named_parameters():\n",
    "                all_param += param.numel()\n",
    "                if param.requires_grad:\n",
    "                    trainable_params += param.numel()\n",
    "            \n",
    "            print(f\"LoRA Parameter Efficiency:\")\n",
    "            print(f\"  Trainable params: {trainable_params:,}\")\n",
    "            print(f\"  All params: {all_param:,}\")\n",
    "            print(f\"  Trainable%: {100 * trainable_params / all_param:.2f}%\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"Extract [CLS] token embeddings\"\"\"\n",
    "        outputs = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        return outputs.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6cc96e",
   "metadata": {},
   "source": [
    "## Traingng helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1457441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    \"\"\"Seed worker for DataLoader\"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "\n",
    "def extract_features(model, dataloader, device, desc=\"Extracting features\"):\n",
    "    \"\"\"Extract DistilBERT features from dataloader\"\"\"\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=desc, leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].cpu().numpy()\n",
    "            \n",
    "            features = model(input_ids, attention_mask)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    return np.vstack(all_features), np.concatenate(all_labels)\n",
    "\n",
    "def evaluate_metrics_encoder(dl, model, device, classification_head):\n",
    "    \"\"\"Evaluate encoder + classification head during fine-tuning\"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].cpu().numpy()\n",
    "            \n",
    "            features = model(input_ids, attention_mask)\n",
    "            logits = classification_head(features)\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            \n",
    "            y_true.extend(labels)\n",
    "            y_pred.extend(preds)\n",
    "    \n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return p, r, f1, acc\n",
    "\n",
    "def save_checkpoint(path, epoch, model, optimizer, scheduler, config, best_f1, seed):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict() if scheduler is not None else None,\n",
    "        \"config\": config,\n",
    "        \"best_val_f1\": best_f1,\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "    tmp = path + \".tmp\"\n",
    "    torch.save(state, tmp)\n",
    "    os.replace(tmp, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0050b",
   "metadata": {},
   "source": [
    "## create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a309373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created:\n",
      "  Train batches: 1026\n",
      "  Val batches: 63\n",
      "  Test batches: 63\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data\n",
    "train_df = pd.read_csv('clean_data_distilbert/train_clean.csv')\n",
    "val_df = pd.read_csv('clean_data_distilbert/val_clean.csv')\n",
    "test_df = pd.read_csv('clean_data_distilbert/test_clean.csv')\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Create datasets\n",
    "train_ds = EmailDataset(train_df[text_col].values, train_df['label'].values, tokenizer, max_length=256)\n",
    "val_ds = EmailDataset(val_df[text_col].values, val_df['label'].values, tokenizer, max_length=256)\n",
    "test_ds = EmailDataset(test_df[text_col].values, test_df['label'].values, tokenizer, max_length=256)\n",
    "\n",
    "# Create DataLoaders\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True,\n",
    "                      num_workers=2, pin_memory=True,\n",
    "                      worker_init_fn=seed_worker, generator=g)\n",
    "val_dl = DataLoader(val_ds, batch_size=128, shuffle=False,\n",
    "                    num_workers=2, pin_memory=True,\n",
    "                    worker_init_fn=seed_worker, generator=g)\n",
    "test_dl = DataLoader(test_ds, batch_size=128, shuffle=False,\n",
    "                     num_workers=2, pin_memory=True,\n",
    "                     worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "print(f\"DataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_dl)}\")\n",
    "print(f\"  Val batches: {len(val_dl)}\")\n",
    "print(f\"  Test batches: {len(test_dl)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30235ba4",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78986cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "{\n",
      "  \"seed\": 42,\n",
      "  \"batch_size_train\": 64,\n",
      "  \"batch_size_eval\": 128,\n",
      "  \"max_length\": 256,\n",
      "  \"lora_r\": 8,\n",
      "  \"lora_alpha\": 16,\n",
      "  \"lora_dropout\": 0.1,\n",
      "  \"lora_target_modules\": [\n",
      "    \"q_lin\",\n",
      "    \"v_lin\"\n",
      "  ],\n",
      "  \"finetune_epochs\": 3,\n",
      "  \"finetune_lr\": 0.001,\n",
      "  \"finetune_weight_decay\": 1e-05,\n",
      "  \"patience\": 3,\n",
      "  \"lr_max_iter\": 1000,\n",
      "  \"lr_C\": 1.0,\n",
      "  \"lr_solver\": \"lbfgs\",\n",
      "  \"num_classes\": 2\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"batch_size_train\": 64,\n",
    "    \"batch_size_eval\": 128,\n",
    "    \"max_length\": 256,\n",
    "    \n",
    "    # LoRA config\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"lora_target_modules\": [\"q_lin\", \"v_lin\"],\n",
    "    \n",
    "    # Fine-tuning config\n",
    "    \"finetune_epochs\": 3,\n",
    "    \"finetune_lr\": 1e-3,\n",
    "    \"finetune_weight_decay\": 1e-5,\n",
    "    \"patience\": 3,\n",
    "    \n",
    "    # Logistic Regression config\n",
    "    \"lr_max_iter\": 1000,\n",
    "    \"lr_C\": 1.0,\n",
    "    \"lr_solver\": \"lbfgs\",\n",
    "    \n",
    "    \"num_classes\": 2,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(json.dumps(CONFIG, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4b7b29",
   "metadata": {},
   "source": [
    "## Initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c385543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Parameter Efficiency:\n",
      "  Trainable params: 147,456\n",
      "  All params: 66,510,336\n",
      "  Trainable%: 0.22%\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    target_modules=CONFIG[\"lora_target_modules\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION\n",
    ")\n",
    "\n",
    "# Initialize encoder\n",
    "encoder = DistilBERTLoRAEncoder(lora_config=lora_config, use_lora=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bbe9c5",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aca30a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 1: FINE-TUNING DISTILBERT WITH LORA\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m logits = classification_head(features)\n\u001b[32m     46\u001b[39m loss = criterion(logits, labels)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     50\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa4213/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa4213/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa4213/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1: FINE-TUNING DISTILBERT WITH LORA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Temporary classification head for fine-tuning\n",
    "classification_head = nn.Linear(768, CONFIG[\"num_classes\"]).to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(encoder.parameters()) + list(classification_head.parameters()),\n",
    "    lr=CONFIG[\"finetune_lr\"],\n",
    "    weight_decay=CONFIG[\"finetune_weight_decay\"]\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "best_val_f1 = -1.0\n",
    "best_val_loss = float(\"inf\")\n",
    "no_improve = 0\n",
    "finetune_history = []\n",
    "\n",
    "CKPT_DIR = \"checkpoints_distilbert\"\n",
    "BEST_CKPT_DIR = \"best_ckpts_distilbert\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "os.makedirs(BEST_CKPT_DIR, exist_ok=True)\n",
    "\n",
    "for epoch in range(1, CONFIG[\"finetune_epochs\"] + 1):\n",
    "    # Training\n",
    "    encoder.train()\n",
    "    classification_head.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    train_pbar = tqdm(train_dl, desc=f\"Epoch {epoch:02d} [Train]\", leave=False)\n",
    "    for batch in train_pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        features = encoder(input_ids, attention_mask)\n",
    "        logits = classification_head(features)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    \n",
    "    train_loss_avg = running_loss / len(train_dl)\n",
    "    \n",
    "    # Validation\n",
    "    encoder.eval()\n",
    "    classification_head.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_dl, desc=f\"Epoch {epoch:02d} [Val]\", leave=False)\n",
    "        for batch in val_pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            features = encoder(input_ids, attention_mask)\n",
    "            logits = classification_head(features)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_pbar.set_postfix(vloss=f\"{loss.item():.4f}\")\n",
    "    \n",
    "    val_loss_avg = val_loss / len(val_dl)\n",
    "    p, r, val_f1, acc = evaluate_metrics_encoder(val_dl, encoder, device, classification_head)\n",
    "    scheduler.step(val_loss_avg)\n",
    "    \n",
    "    # Record metrics\n",
    "    finetune_history.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": float(train_loss_avg),\n",
    "        \"val_loss\": float(val_loss_avg),\n",
    "        \"val_precision\": float(p),\n",
    "        \"val_recall\": float(r),\n",
    "        \"val_F1\": float(val_f1),\n",
    "        \"val_accuracy\": float(acc),\n",
    "    })\n",
    "    \n",
    "    print(f\"epoch {epoch:02d} | train_loss={train_loss_avg:.4f} | val_loss={val_loss_avg:.4f} | \"\n",
    "          f\"P={p:.4f} | R={r:.4f} | F1={val_f1:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    save_checkpoint(\n",
    "        os.path.join(CKPT_DIR, f\"finetune_epoch_{epoch:02d}.pt\"),\n",
    "        epoch, encoder, optimizer, scheduler, CONFIG, best_val_f1, SEED\n",
    "    )\n",
    "    \n",
    "    # Track best model\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        save_checkpoint(\n",
    "            os.path.join(BEST_CKPT_DIR, \"best_finetune.pt\"),\n",
    "            epoch, encoder, optimizer, scheduler, CONFIG, best_val_f1, SEED\n",
    "        )\n",
    "        print(f\"  ✓ New best F1: {best_val_f1:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss_avg < best_val_loss - 1e-5:\n",
    "        best_val_loss = val_loss_avg\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= CONFIG[\"patience\"]:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "# Save fine-tuning history\n",
    "finetune_df = pd.DataFrame(finetune_history)\n",
    "finetune_df.to_csv(os.path.join(CKPT_DIR, \"finetune_history.csv\"), index=False)\n",
    "print(f\"\\nBest validation F1 during fine-tuning: {best_val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370322f",
   "metadata": {},
   "source": [
    "## Extract features for log reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd3006",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2: EXTRACTING FEATURES FOR LOGISTIC REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract features from fine-tuned encoder\n",
    "X_train, y_train = extract_features(encoder, train_dl, device, \"Train features\")\n",
    "X_val, y_val = extract_features(encoder, val_dl, device, \"Val features\")\n",
    "\n",
    "print(f\"\\nFeature shapes:\")\n",
    "print(f\"  Train: {X_train.shape}\")\n",
    "print(f\"  Val: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afbe9e4",
   "metadata": {},
   "source": [
    "## train log reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebdf0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 3: TRAINING LOGISTIC REGRESSION CLASSIFIER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train Logistic Regression\n",
    "lr_classifier = LogisticRegression(\n",
    "    max_iter=CONFIG[\"lr_max_iter\"],\n",
    "    random_state=SEED,\n",
    "    class_weight='balanced',\n",
    "    solver=CONFIG[\"lr_solver\"],\n",
    "    C=CONFIG[\"lr_C\"]\n",
    ")\n",
    "\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Quick validation check\n",
    "y_val_pred = lr_classifier.predict(X_val)\n",
    "p, r, f1, _ = precision_recall_fscore_support(y_val, y_val_pred, average=\"binary\", zero_division=0)\n",
    "acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"\\nLogistic Regression - Validation Set:\")\n",
    "print(f\"  Accuracy:  {acc:.4f}\")\n",
    "print(f\"  Precision: {p:.4f}\")\n",
    "print(f\"  Recall:    {r:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb05b7c5",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2362c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save LoRA weights\n",
    "encoder.distilbert.save_pretrained(os.path.join(BEST_CKPT_DIR, \"lora_weights\"))\n",
    "print(f\"✓ LoRA weights saved to {BEST_CKPT_DIR}/lora_weights/\")\n",
    "\n",
    "# Save Logistic Regression\n",
    "lr_path = os.path.join(BEST_CKPT_DIR, \"logistic_regression.pkl\")\n",
    "joblib.dump(lr_classifier, lr_path)\n",
    "print(f\"✓ Logistic Regression saved to {lr_path}\")\n",
    "\n",
    "# Save tokenizer config\n",
    "tokenizer_config = {\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"max_length\": CONFIG[\"max_length\"]\n",
    "}\n",
    "with open(os.path.join(BEST_CKPT_DIR, \"tokenizer_config.json\"), \"w\") as f:\n",
    "    json.dump(tokenizer_config, f, indent=2)\n",
    "\n",
    "# Create manifest\n",
    "manifest = {\n",
    "    \"seed\": CONFIG[\"seed\"],\n",
    "    \"max_length\": CONFIG[\"max_length\"],\n",
    "    \"lora_r\": CONFIG[\"lora_r\"],\n",
    "    \"lora_alpha\": CONFIG[\"lora_alpha\"],\n",
    "    \"lora_dropout\": CONFIG[\"lora_dropout\"],\n",
    "    \"lora_target_modules\": CONFIG[\"lora_target_modules\"],\n",
    "    \"finetune_epochs\": len(finetune_df),\n",
    "    \"finetune_lr\": CONFIG[\"finetune_lr\"],\n",
    "    \"num_classes\": CONFIG[\"num_classes\"],\n",
    "    \"best_val_f1\": float(best_val_f1),\n",
    "    \"lora_weights_path\": os.path.join(BEST_CKPT_DIR, \"lora_weights\"),\n",
    "    \"lr_model_path\": lr_path,\n",
    "    \"best_finetune_ckpt\": os.path.join(BEST_CKPT_DIR, \"best_finetune.pt\"),\n",
    "}\n",
    "\n",
    "with open(os.path.join(BEST_CKPT_DIR, \"manifest.json\"), \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"✓ Manifest saved to {BEST_CKPT_DIR}/manifest.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bea245",
   "metadata": {},
   "source": [
    "## summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e56ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*20 + \"TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n1. Data Processing:\")\n",
    "print(f\"   ✓ Train samples: {len(train_df)}\")\n",
    "print(f\"   ✓ Validation samples: {len(val_df)}\")\n",
    "print(f\"   ✓ Test samples: {len(test_df)}\")\n",
    "\n",
    "print(f\"\\n2. Fine-tuning (LoRA):\")\n",
    "print(f\"   ✓ Epochs trained: {len(finetune_df)}\")\n",
    "print(f\"   ✓ Best validation F1: {best_val_f1:.4f}\")\n",
    "\n",
    "print(f\"\\n3. Logistic Regression:\")\n",
    "print(f\"   ✓ Features: 768-dimensional DistilBERT embeddings\")\n",
    "print(f\"   ✓ Validation F1: {f1:.4f}\")\n",
    "\n",
    "print(f\"\\n4. Saved Files:\")\n",
    "print(f\"   ✓ {BEST_CKPT_DIR}/lora_weights/\")\n",
    "print(f\"   ✓ {BEST_CKPT_DIR}/logistic_regression.pkl\")\n",
    "print(f\"   ✓ {BEST_CKPT_DIR}/manifest.json\")\n",
    "print(f\"   ✓ {BEST_CKPT_DIR}/tokenizer_config.json\")\n",
    "print(f\"   ✓ {CKPT_DIR}/finetune_history.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*10 + \"Model saved! Ready for downstream evaluation.\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Display training history\n",
    "print(\"\\nFine-tuning History:\")\n",
    "print(finetune_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa4213",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
